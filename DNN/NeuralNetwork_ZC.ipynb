{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS155 Miniproject 1\n",
    "\n",
    "zchen@caltech.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis via Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final validation data set\n",
    "FV_data = np.loadtxt('../data/test_data.txt',delimiter=' ',skiprows=1)\n",
    "# Load input training data set\n",
    "train_data = np.loadtxt('../data/training_data.txt',delimiter=' ',skiprows=1)\n",
    "utrain_data = np.unique(train_data,axis=0)\n",
    "\n",
    "# Get header words list\n",
    "f = open('../data/test_data.txt','r')\n",
    "words = np.array(f.readline().split())\n",
    "f.close()\n",
    "\n",
    "# Splot y_train and x_train from training set\n",
    "x_tall = train_data[:,1:]\n",
    "y_tall = train_data[:,0]\n",
    "\n",
    "x_uall = utrain_data[:,1:]\n",
    "y_uall = utrain_data[:,0]\n",
    "\n",
    "# One hot encode categories\n",
    "y_tall = keras.utils.np_utils.to_categorical(y_tall)\n",
    "y_uall = keras.utils.np_utils.to_categorical(y_uall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions for Neural network debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate DNN of given depth and width\n",
    "def getModel(layers,Pdrop):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layers[0],input_shape=(1000,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(Pdrop))\n",
    "    for i in layers[1:]:\n",
    "        model.add(Dense(i))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "    # predicting probabilities of each of the 2 classes\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "# undo one hot encoding\n",
    "def Unencode(out):\n",
    "    ypred = out[:,0] < out[:,1]\n",
    "    return ypred\n",
    "\n",
    "# Function to get explicit model accuracy from softmax\n",
    "def getAccuracy(model,xt,yt):\n",
    "    out = model.predict(xt)\n",
    "    ypred = Unencode(out)\n",
    "    ytrue = Unencode(yt)\n",
    "    acc = 1.0*np.sum(ypred == ytrue)/len(ytrue)\n",
    "    return acc\n",
    "\n",
    "# Function to get bag of words which were misclassified\n",
    "def getBagOfWords(xtrain,ypred,ytrue,words):\n",
    "    out = []\n",
    "    ypredu = Unencode(ypred).astype(int)\n",
    "    ytrueu = Unencode(ytrue).astype(int)\n",
    "    # Get locations of bag of words which were misclassified\n",
    "    idx = np.arange(0,len(ypredu))\n",
    "    idxErr = idx[ypredu!=ytrueu]\n",
    "    Xerr = xtrain[ypredu!=ytrueu]\n",
    "    j = 0\n",
    "    for i in Xerr:\n",
    "        out.append([ytrue[idxErr[j]],ypred[idxErr[j]],words[i>0],i[i>0]])\n",
    "        j=j+1\n",
    "    return out\n",
    "\n",
    "# Function to write final predictions\n",
    "def writeResults(ypred):\n",
    "    f = open('DNN_submission.txt','w')\n",
    "    f.write('Id,Prediction\\n')\n",
    "    for i in range(0,len(ypred)):\n",
    "        f.write(str(i+1)+','+str(int(ypred[i]))+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural network model for bag of words predictor\n",
    "\n",
    "Initial testing of single DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 9s 590us/step - loss: 0.4957 - acc: 0.7658 - val_loss: 0.3615 - val_acc: 0.8400\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 206us/step - loss: 0.3180 - acc: 0.8627 - val_loss: 0.3440 - val_acc: 0.8553\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 193us/step - loss: 0.2556 - acc: 0.8916 - val_loss: 0.3716 - val_acc: 0.8562\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 173us/step - loss: 0.2084 - acc: 0.9160 - val_loss: 0.3898 - val_acc: 0.8507\n",
      "4000/4000 [==============================] - 2s 526us/step\n"
     ]
    }
   ],
   "source": [
    "# Split the training data k-fold number of ways for k-fold validation of the learning algorithm\n",
    "k=5\n",
    "kf = sklearn.model_selection.KFold(n_splits=k)\n",
    "inds = [ind for ind in kf.split(x_tall, y_tall)]\n",
    "\n",
    "i=0\n",
    "train,val = inds[0]\n",
    "# Training and validation data for k fold cross validation\n",
    "Xtrain = x_tall[train]\n",
    "Ytrain = y_tall[train]\n",
    "Xval = x_tall[val]\n",
    "Yval = y_tall[val]\n",
    "\n",
    "# Define the DNN model\n",
    "model = getModel([500,250,125],0.4)\n",
    "\n",
    "# Compile it and fit\n",
    "model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=4,verbose=1,validation_data=(Xval, Yval))\n",
    "ypred = model.predict(Xval,batch_size=2**8,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 1.]), array([0.6994479 , 0.30055207], dtype=float32), array(['thi', 'veri', 'get', 'onli', 'look', 'want', 'see', 'mani', 'new',\n",
      "       'peopl', 'still', 'need', 'two', 'feel', 'start', 'long', 'listen',\n",
      "       'excel', 'enough', 'person', 'cover', 'almost', 'scene', 'instead',\n",
      "       'famili', 'sever', 'hour', 'els', 'fine', 'talk', 'american',\n",
      "       'entir', 'lack', 'impress', 'state', 'avail', 'certainli',\n",
      "       'student', 'danc', 'parent', 'critic', 'centuri', 'train',\n",
      "       'aspect'], dtype='<U12'), array([3., 2., 1., 1., 2., 1., 2., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       2., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
      "       1., 1., 2., 2., 6., 1., 1., 1., 1., 1.])]\n"
     ]
    }
   ],
   "source": [
    "# Debugging bag of words\n",
    "out = getBagOfWords(Xval,ypred,Yval,words)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method  validation for single DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the DNN model\n",
    "def getDNNCross(k,x_tall,y_tall):\n",
    "    # Storage for k fold cross validation    \n",
    "    trainErr = []\n",
    "    testErr = []\n",
    "    acc = []\n",
    "    \n",
    "    # Split the training data k-fold number of ways for k-fold validation of the learning algorithm\n",
    "    kf = sklearn.model_selection.KFold(n_splits=k)\n",
    "    inds = [ind for ind in kf.split(x_tall, y_tall)]\n",
    "    \n",
    "    i=0\n",
    "    for train,val in inds:\n",
    "        # Training and validation data for k fold cross validation\n",
    "        Xtrain = x_tall[train]\n",
    "        Ytrain = y_tall[train]\n",
    "        Xval = x_tall[val]\n",
    "        Yval = y_tall[val]\n",
    "    \n",
    "        # Define the DNN model\n",
    "        model = getModel([500,250,125],0.4)\n",
    "        \n",
    "        # Compile it and fit\n",
    "        model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "        model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=10,verbose=1,validation_data=(Xval, Yval))\n",
    "        \n",
    "        # store training and test error\n",
    "        trainErr.append(model.evaluate(x=Xtrain, y=Ytrain))\n",
    "        testErr.append(model.evaluate(x=Xval, y=Yval))\n",
    "        acc.append(getAccuracy(model,Xval,Yval))\n",
    "\n",
    "        # Status output\n",
    "        print('k-iteration = ',i)\n",
    "        i=i+1\n",
    "\n",
    "    trainErr = np.array(trainErr)\n",
    "    testErr = np.array(testErr)        \n",
    "    print('\\n trainErr')\n",
    "    print(trainErr)\n",
    "    print('avg trainErr',np.mean(trainErr,axis=0))\n",
    "    print('\\n testErr')\n",
    "    print(testErr)\n",
    "    print('avg testErr',np.mean(testErr,axis=0))\n",
    "    print('\\n Accuracy')\n",
    "    print(acc)\n",
    "    print('avg acc',np.mean(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 14s 904us/step - loss: 0.4877 - acc: 0.7672 - val_loss: 0.3531 - val_acc: 0.8415\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 4s 232us/step - loss: 0.3224 - acc: 0.8587 - val_loss: 0.3569 - val_acc: 0.8460\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 3s 188us/step - loss: 0.2588 - acc: 0.8908 - val_loss: 0.3846 - val_acc: 0.8503\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 3s 183us/step - loss: 0.2115 - acc: 0.9119 - val_loss: 0.3895 - val_acc: 0.8470\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 3s 180us/step - loss: 0.1695 - acc: 0.9320 - val_loss: 0.4384 - val_acc: 0.8445\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 3s 202us/step - loss: 0.1379 - acc: 0.9449 - val_loss: 0.4329 - val_acc: 0.8460\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 3s 185us/step - loss: 0.1075 - acc: 0.9587 - val_loss: 0.5665 - val_acc: 0.8377\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 3s 180us/step - loss: 0.0931 - acc: 0.9644 - val_loss: 0.5285 - val_acc: 0.8445\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 3s 179us/step - loss: 0.0758 - acc: 0.9719 - val_loss: 0.6146 - val_acc: 0.8365\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 3s 180us/step - loss: 0.0579 - acc: 0.9787 - val_loss: 0.6198 - val_acc: 0.8437\n",
      "16000/16000 [==============================] - 4s 256us/step\n",
      "4000/4000 [==============================] - 1s 256us/step\n",
      "k-iteration =  0\n",
      "\n",
      " trainErr\n",
      "[[0.00926458 0.99825   ]]\n",
      "avg trainErr [0.00926458 0.99825   ]\n",
      "\n",
      " testErr\n",
      "[[0.61978289 0.84375   ]]\n",
      "avg testErr [0.61978289 0.84375   ]\n",
      "\n",
      " Accuracy\n",
      "[0.84375]\n",
      "avg acc 0.84375\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 16s 973us/step - loss: 0.4783 - acc: 0.7747 - val_loss: 0.3648 - val_acc: 0.8418\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 3s 195us/step - loss: 0.3192 - acc: 0.8616 - val_loss: 0.3585 - val_acc: 0.8495\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 3s 193us/step - loss: 0.2589 - acc: 0.8905 - val_loss: 0.3907 - val_acc: 0.8380\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 3s 179us/step - loss: 0.2114 - acc: 0.9136 - val_loss: 0.4053 - val_acc: 0.8490\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 3s 178us/step - loss: 0.1689 - acc: 0.9334 - val_loss: 0.4234 - val_acc: 0.8480\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 3s 182us/step - loss: 0.1374 - acc: 0.9461 - val_loss: 0.4857 - val_acc: 0.8430\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 3s 192us/step - loss: 0.1092 - acc: 0.9585 - val_loss: 0.5122 - val_acc: 0.8470\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 3s 204us/step - loss: 0.0884 - acc: 0.9671 - val_loss: 0.5473 - val_acc: 0.8468\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 3s 195us/step - loss: 0.0728 - acc: 0.9729 - val_loss: 0.6542 - val_acc: 0.8260\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 3s 207us/step - loss: 0.0617 - acc: 0.9769 - val_loss: 0.6305 - val_acc: 0.8463\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'append'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-45e6099be49c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetDNNCross\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_tall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_tall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-137-ef74a8c5f060>\u001b[0m in \u001b[0;36mgetDNNCross\u001b[0;34m(k, x_tall, y_tall)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# store training and test error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtrainErr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mYtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mtestErr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mXval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mYval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0macc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetAccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mXval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mYval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'append'"
     ]
    }
   ],
   "source": [
    "getDNNCross(5,x_tall,y_tall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying Random Forest of weakly trained Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN  0\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "# Split the training data k-fold number of ways for k-fold validation of the learning algorithm\n",
    "k=5\n",
    "kf = sklearn.model_selection.KFold(n_splits=k)\n",
    "inds = [ind for ind in kf.split(x_uall, y_uall)]\n",
    "\n",
    "train,val = inds[0]\n",
    "# Training and validation data for k fold cross validation\n",
    "Xt = x_uall[train]\n",
    "Yt = y_uall[train]\n",
    "Xval = x_uall[val]\n",
    "Yval = y_uall[val]\n",
    "\n",
    "# Specify number of Neural networks to train\n",
    "N_models = 10\n",
    "Predictions = []\n",
    "TrainErr = []\n",
    "TestErr = []\n",
    "\n",
    "# Define the DNN model\n",
    "for i in range(0,N_models):\n",
    "    print('Training DNN ',i)\n",
    "    \n",
    "    # Randomly sample from training set for each random tree\n",
    "    idx = np.random.choice(len(Xtrain),len(Xtrain))\n",
    "    Xtrain = Xt[idx]\n",
    "    Ytrain = Yt[idx]\n",
    "\n",
    "    # Generate model and fit to data\n",
    "    model = getModel([500,250,125],0.4)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "    model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=2)\n",
    "\n",
    "    # Store the models\n",
    "    TrainErr.append(model.evaluate(x=Xtrain, y=Ytrain))\n",
    "    TestErr.append(model.evaluate(x=Xval, y=Yval))\n",
    "    # Use weakly trained model to predict and store predictions\n",
    "    ypred = model.predict(Xval,batch_size=2**8)\n",
    "    Predictions.append(ypred)\n",
    "\n",
    "Predictions = np.array(Predictions)\n",
    "TrainErr = np.array(TrainErr)\n",
    "TestErr = np.array(TestErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training err \\n',TrainErr)\n",
    "print('test err \\n',TestErr)\n",
    "\n",
    "ypred = []\n",
    "for i in Predictions:\n",
    "    ypred.append(Unencode(i).astype(int))\n",
    "\n",
    "# Get mean and standard deviation of samples\n",
    "ypmean=np.mean(ypred,axis=0)\n",
    "std=np.std(ypred,axis=0)\n",
    "\n",
    "print('Number of samples where stdev > 0, ',np.sum(std>0))\n",
    "ypred = (ypmean > 0.5).astype(int)\n",
    "ytrue = Unencode(Yval).astype(int)\n",
    "\n",
    "acc = 1.0*np.sum(ypred == ytrue)/len(ytrue)\n",
    "print('Final accuracy of random forest = ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random neuron forest\n",
    "def getDNNForestCross(k,N_models,x_tall,y_tall):\n",
    "    # Storage for k fold cross validation\n",
    "    facc = []\n",
    "    \n",
    "    # Split the training data k-fold number of ways for k-fold validation of the learning algorithm\n",
    "    k=5\n",
    "    kf = sklearn.model_selection.KFold(n_splits=k)\n",
    "    inds = [ind for ind in kf.split(x_tall, y_tall)]\n",
    "\n",
    "    j=0\n",
    "    for train,val in inds:\n",
    "        print('Cross fold validation ',j+1,'/',k)\n",
    "        # Training and validation data for k fold cross validation\n",
    "        Xtrain = x_tall[train]\n",
    "        Ytrain = y_tall[train]\n",
    "        Xval = x_tall[val]\n",
    "        Yval = y_tall[val]\n",
    "\n",
    "        # Store predictions from each tree in DNN forest\n",
    "        Predictions = []\n",
    "        \n",
    "        # Define the DNN model\n",
    "        for i in range(0,N_models):\n",
    "            print('Training DNN ',i)\n",
    "            model = getModel([500,250,125],0)\n",
    "            # Compile it and fit\n",
    "            model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "            model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=2,verbose=1)\n",
    "\n",
    "            # Use weakly trained model to predict and store predictions\n",
    "            ypred = model.predict(Xval,batch_size=2**8,verbose=1)\n",
    "            Predictions.append(ypred)\n",
    "\n",
    "        Predictions = np.array(Predictions)\n",
    "        \n",
    "        ypred = []\n",
    "        for i in Predictions:\n",
    "            ypred.append(Unencode(i).astype(int))\n",
    "\n",
    "        # Get mean and standard deviation of samples\n",
    "        ypmean=np.mean(ypred,axis=0)\n",
    "        std=np.std(ypred,axis=0)\n",
    "        print('Number of samples where stdev > 0, ',np.sum(std>0))\n",
    "        \n",
    "        # Compute accuracy of predictions\n",
    "        ypred = (ypmean > 0.5).astype(int)\n",
    "        ytrue = Unencode(Yval).astype(int)\n",
    "        acc = 1.0*np.sum(ypred == ytrue)/len(ytrue)\n",
    "        print('Final accuracy of random forest = ',acc)\n",
    "        j=j+1\n",
    "        \n",
    "        facc.append(acc)\n",
    "        \n",
    "    print('avg Accuracy of all random forests',np.mean(facc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross fold validation  0 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 13s 797us/step - loss: 0.4342 - acc: 0.8000\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 154us/step - loss: 0.1825 - acc: 0.9311\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 844us/step - loss: 0.4393 - acc: 0.7969\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 181us/step - loss: 0.1899 - acc: 0.9297\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 13s 813us/step - loss: 0.4315 - acc: 0.8037\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 197us/step - loss: 0.1785 - acc: 0.9337\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 849us/step - loss: 0.4482 - acc: 0.7908\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 161us/step - loss: 0.1801 - acc: 0.9327\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 914us/step - loss: 0.4264 - acc: 0.8039\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 167us/step - loss: 0.1872 - acc: 0.9296\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 868us/step - loss: 0.4354 - acc: 0.8004\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.1803 - acc: 0.9325\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 901us/step - loss: 0.4466 - acc: 0.7948\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1833 - acc: 0.9324 1s - los\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 877us/step - loss: 0.4276 - acc: 0.8001\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 193us/step - loss: 0.1797 - acc: 0.9323\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 926us/step - loss: 0.4320 - acc: 0.8007\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 168us/step - loss: 0.1847 - acc: 0.9292\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 918us/step - loss: 0.4365 - acc: 0.7991\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.1814 - acc: 0.9316\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Number of samples where stdev > 0,  1412\n",
      "Final accuracy of random forest =  0.85875\n",
      "Cross fold validation  1 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 953us/step - loss: 0.4319 - acc: 0.8009\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 170us/step - loss: 0.1727 - acc: 0.9356\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 895us/step - loss: 0.4277 - acc: 0.8003\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 155us/step - loss: 0.1778 - acc: 0.9345\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 956us/step - loss: 0.4280 - acc: 0.7999\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 178us/step - loss: 0.1737 - acc: 0.9371\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 974us/step - loss: 0.4380 - acc: 0.7984\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 179us/step - loss: 0.1732 - acc: 0.9376\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 926us/step - loss: 0.4274 - acc: 0.8024\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 174us/step - loss: 0.1778 - acc: 0.9344\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 942us/step - loss: 0.4268 - acc: 0.8018\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 181us/step - loss: 0.1781 - acc: 0.9345\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 954us/step - loss: 0.4271 - acc: 0.8026\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 183us/step - loss: 0.1814 - acc: 0.9299\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 922us/step - loss: 0.4383 - acc: 0.7955\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 157us/step - loss: 0.1805 - acc: 0.9323\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 985us/step - loss: 0.4259 - acc: 0.8011\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 203us/step - loss: 0.1843 - acc: 0.9308\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 933us/step - loss: 0.4337 - acc: 0.8018\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 214us/step - loss: 0.1791 - acc: 0.9334\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Number of samples where stdev > 0,  1477\n",
      "Final accuracy of random forest =  0.8565\n",
      "Cross fold validation  2 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 943us/step - loss: 0.4328 - acc: 0.7986\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 176us/step - loss: 0.1779 - acc: 0.9344\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 916us/step - loss: 0.4273 - acc: 0.8040\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 172us/step - loss: 0.1764 - acc: 0.9368\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 926us/step - loss: 0.4320 - acc: 0.8034\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 174us/step - loss: 0.1699 - acc: 0.9351\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4280 - acc: 0.8054\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 215us/step - loss: 0.1779 - acc: 0.9364\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 949us/step - loss: 0.4283 - acc: 0.8006\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1789 - acc: 0.9339\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 999us/step - loss: 0.4344 - acc: 0.7976\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.1716 - acc: 0.9378\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 934us/step - loss: 0.4269 - acc: 0.8023\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1805 - acc: 0.9324\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 940us/step - loss: 0.4355 - acc: 0.7994\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 170us/step - loss: 0.1769 - acc: 0.9353\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4364 - acc: 0.8008\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 194us/step - loss: 0.1781 - acc: 0.9326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4347 - acc: 0.7999\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 158us/step - loss: 0.1776 - acc: 0.9346\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Number of samples where stdev > 0,  1377\n",
      "Final accuracy of random forest =  0.84375\n",
      "Cross fold validation  3 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 979us/step - loss: 0.4330 - acc: 0.7994\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 177us/step - loss: 0.1770 - acc: 0.9359\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 948us/step - loss: 0.4278 - acc: 0.8039\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 143us/step - loss: 0.1769 - acc: 0.9340\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4365 - acc: 0.7973\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 167us/step - loss: 0.1760 - acc: 0.9360\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4252 - acc: 0.8042\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.1731 - acc: 0.9356\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 966us/step - loss: 0.4374 - acc: 0.7981\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 182us/step - loss: 0.1804 - acc: 0.9327\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4335 - acc: 0.7999\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 151us/step - loss: 0.1743 - acc: 0.9341\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4247 - acc: 0.8025\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 5s 301us/step - loss: 0.1781 - acc: 0.9342\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4332 - acc: 0.8029\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 4s 241us/step - loss: 0.1710 - acc: 0.9393\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4344 - acc: 0.8004\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 193us/step - loss: 0.1707 - acc: 0.9379 1s - loss: \n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4299 - acc: 0.8013\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 174us/step - loss: 0.1760 - acc: 0.9359\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Number of samples where stdev > 0,  1400\n",
      "Final accuracy of random forest =  0.84675\n",
      "Cross fold validation  4 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4308 - acc: 0.7984\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 167us/step - loss: 0.1777 - acc: 0.9331\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 999us/step - loss: 0.4294 - acc: 0.8018\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.1806 - acc: 0.9347\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4283 - acc: 0.8021\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.1834 - acc: 0.9310\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4428 - acc: 0.7974\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.1747 - acc: 0.9364\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4416 - acc: 0.7973\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 157us/step - loss: 0.1758 - acc: 0.9333\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4360 - acc: 0.8007\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 170us/step - loss: 0.1778 - acc: 0.9332\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4361 - acc: 0.7977\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 178us/step - loss: 0.1772 - acc: 0.9343\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4423 - acc: 0.7996\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 181us/step - loss: 0.1736 - acc: 0.9357\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4370 - acc: 0.7953\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 161us/step - loss: 0.1745 - acc: 0.9354\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4249 - acc: 0.8009\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.1794 - acc: 0.9340\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Number of samples where stdev > 0,  1661\n",
      "Final accuracy of random forest =  0.8485\n",
      "avg Accuracy of all random forests 0.8508500000000001\n"
     ]
    }
   ],
   "source": [
    "getDNNForestCross(5,10,x_tall,y_tall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output final predictions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN  0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected activation_723 to have shape (2,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-5d9c778c63ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Compile it and fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'RMSprop'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_uall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_uall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Use weakly trained model to predict and store predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mypred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFV_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1592\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1593\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1594\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1595\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1431\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1432\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    118\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    121\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected activation_723 to have shape (2,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "# Specify number of Neural networks to train\n",
    "N_models = 20\n",
    "Predictions = []\n",
    "\n",
    "# Define the DNN model\n",
    "for i in range(0,N_models):\n",
    "    print('Training DNN ',i)\n",
    "    model = getModel([500,250,125],0)\n",
    "    # Compile it and fit\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "    model.fit(x_uall, y_uall, batch_size=2**8, epochs=2)\n",
    "    # Use weakly trained model to predict and store predictions\n",
    "    ypred = model.predict(FV_data,batch_size=2**8)\n",
    "    Predictions.append(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions = np.array(Predictions)\n",
    "\n",
    "ypred = []\n",
    "for i in Predictions:\n",
    "    ypred.append(Unencode(i).astype(int))\n",
    "\n",
    "# Get mean and standard deviation of samples\n",
    "ypmean=np.mean(ypred,axis=0)\n",
    "std=np.std(ypred,axis=0)\n",
    "print('Number of samples where stdev > 0, ',np.sum(std>0))\n",
    "\n",
    "# Compute final predictions and output it\n",
    "ypred = (ypmean > 0.5).astype(int)\n",
    "writeResults(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
