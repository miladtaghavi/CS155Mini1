{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS155 Miniproject 1\n",
    "\n",
    "zchen@caltech.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis via Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final validation data set\n",
    "FV_data = np.loadtxt('../data/test_data.txt',delimiter=' ',skiprows=1)\n",
    "# Load input training data set\n",
    "train_data = np.loadtxt('../data/training_data.txt',delimiter=' ',skiprows=1)\n",
    "utrain_data = np.unique(train_data,axis=0)\n",
    "\n",
    "# Get header words list\n",
    "f = open('../data/test_data.txt','r')\n",
    "words = np.array(f.readline().split())\n",
    "f.close()\n",
    "\n",
    "# Splot y_train and x_train from training set\n",
    "x_tall = train_data[:,1:]\n",
    "y_tall = train_data[:,0]\n",
    "\n",
    "x_uall = utrain_data[:,1:]\n",
    "y_uall = utrain_data[:,0]\n",
    "\n",
    "# One hot encode categories\n",
    "y_tall = keras.utils.np_utils.to_categorical(y_tall)\n",
    "y_uall = keras.utils.np_utils.to_categorical(y_uall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions for Neural network debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate DNN of given depth and width\n",
    "def getModel(layers,Pdrop):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layers[0],input_shape=(1000,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(Pdrop))\n",
    "    for i in layers[1:]:\n",
    "        model.add(Dense(i))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "    # predicting probabilities of each of the 2 classes\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "# undo one hot encoding\n",
    "def Unencode(out):\n",
    "    ypred = out[:,0] < out[:,1]\n",
    "    return ypred\n",
    "\n",
    "# Function to get explicit model accuracy from softmax\n",
    "def getAccuracy(model,xt,yt):\n",
    "    out = model.predict(xt)\n",
    "    ypred = Unencode(out)\n",
    "    ytrue = Unencode(yt)\n",
    "    acc = 1.0*np.sum(ypred == ytrue)/len(ytrue)\n",
    "    return acc\n",
    "\n",
    "# Function to get bag of words which were misclassified\n",
    "def getBagOfWords(xtrain,ypred,ytrue,words):\n",
    "    out = []\n",
    "    ypredu = Unencode(ypred).astype(int)\n",
    "    ytrueu = Unencode(ytrue).astype(int)\n",
    "    # Get locations of bag of words which were misclassified\n",
    "    idx = np.arange(0,len(ypredu))\n",
    "    idxErr = idx[ypredu!=ytrueu]\n",
    "    Xerr = xtrain[ypredu!=ytrueu]\n",
    "    j = 0\n",
    "    for i in Xerr:\n",
    "        out.append([ytrue[idxErr[j]],ypred[idxErr[j]],words[i>0],i[i>0]])\n",
    "        j=j+1\n",
    "    return out\n",
    "\n",
    "# Function to write final predictions\n",
    "def writeResults(ypred):\n",
    "    f = open('DNN_submission.txt','w')\n",
    "    f.write('Id,Prediction\\n')\n",
    "    for i in range(0,len(ypred)):\n",
    "        f.write(str(i+1)+','+str(int(ypred[i]))+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural network model for bag of words predictor\n",
    "\n",
    "Initial testing of single DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 4s 236us/step - loss: 0.4844 - acc: 0.7659 - val_loss: 0.3458 - val_acc: 0.8495\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 186us/step - loss: 0.3172 - acc: 0.8629 - val_loss: 0.3455 - val_acc: 0.8562\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 203us/step - loss: 0.2608 - acc: 0.8921 - val_loss: 0.3526 - val_acc: 0.8540\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 204us/step - loss: 0.2123 - acc: 0.9129 - val_loss: 0.3760 - val_acc: 0.8518\n",
      "4000/4000 [==============================] - 0s 102us/step\n"
     ]
    }
   ],
   "source": [
    "# Split the training data k-fold number of ways for k-fold validation of the learning algorithm\n",
    "k=5\n",
    "kf = sklearn.model_selection.KFold(n_splits=k)\n",
    "inds = [ind for ind in kf.split(x_tall, y_tall)]\n",
    "\n",
    "i=0\n",
    "train,val = inds[0]\n",
    "# Training and validation data for k fold cross validation\n",
    "Xtrain = x_tall[train]\n",
    "Ytrain = y_tall[train]\n",
    "Xval = x_tall[val]\n",
    "Yval = y_tall[val]\n",
    "\n",
    "# Define the DNN model\n",
    "model = getModel([500,250,125],0.4)\n",
    "\n",
    "# Compile it and fit\n",
    "model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=4,verbose=1,validation_data=(Xval, Yval))\n",
    "ypred = model.predict(Xval,batch_size=2**8,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 14s 899us/step - loss: 0.4477 - acc: 0.7919 - val_loss: 0.3569 - val_acc: 0.8470\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 174us/step - loss: 0.1809 - acc: 0.9316 - val_loss: 0.3912 - val_acc: 0.8380\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 4s 219us/step - loss: 0.0818 - acc: 0.9749 - val_loss: 0.4721 - val_acc: 0.8308\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.0403 - acc: 0.9884 - val_loss: 0.5333 - val_acc: 0.8350\n",
      "16000/16000 [==============================] - 4s 244us/step\n",
      "4000/4000 [==============================] - 1s 246us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 14s 879us/step - loss: 0.4415 - acc: 0.7963 - val_loss: 0.3676 - val_acc: 0.8355\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 183us/step - loss: 0.2023 - acc: 0.9212 - val_loss: 0.3967 - val_acc: 0.8365\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 186us/step - loss: 0.1006 - acc: 0.9661 - val_loss: 0.5461 - val_acc: 0.8123\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 181us/step - loss: 0.0539 - acc: 0.9825 - val_loss: 0.4794 - val_acc: 0.8360\n",
      "16000/16000 [==============================] - 4s 266us/step\n",
      "4000/4000 [==============================] - 1s 260us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "15872/16000 [============================>.] - ETA: 0s - loss: 0.4503 - acc: 0.7884"
     ]
    }
   ],
   "source": [
    "# Doing hyperparameter optimization\n",
    "dp = np.linspace(0,0.9,19)\n",
    "Vacc = []\n",
    "TrainErr = []\n",
    "TestErr = []\n",
    "for p in dp:\n",
    "    model = getModel([500,250,125],p)\n",
    "    # Compile it and fit\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "    model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=4,verbose=1,validation_data=(Xval, Yval))\n",
    "    TrainErr.append(model.evaluate(x=Xtrain, y=Ytrain))\n",
    "    TestErr.append(model.evaluate(x=Xval, y=Yval))\n",
    "TrainErr = np.array(TrainErr)\n",
    "TestErr = np.array(TestErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1,figsize=(5,5))\n",
    "plt.title('Dropout prob. vs accuracy')\n",
    "plt.xlabel('Dropout prob.')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(dp,TrainErr[:,1],label='Training',color='red')\n",
    "plt.plot(dp,TestErr[:,1],label='Validation',color='blue')\n",
    "plt.savefig('DNN_hyperparam.png')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 1.]), array([0.6994479 , 0.30055207], dtype=float32), array(['thi', 'veri', 'get', 'onli', 'look', 'want', 'see', 'mani', 'new',\n",
      "       'peopl', 'still', 'need', 'two', 'feel', 'start', 'long', 'listen',\n",
      "       'excel', 'enough', 'person', 'cover', 'almost', 'scene', 'instead',\n",
      "       'famili', 'sever', 'hour', 'els', 'fine', 'talk', 'american',\n",
      "       'entir', 'lack', 'impress', 'state', 'avail', 'certainli',\n",
      "       'student', 'danc', 'parent', 'critic', 'centuri', 'train',\n",
      "       'aspect'], dtype='<U12'), array([3., 2., 1., 1., 2., 1., 2., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       2., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
      "       1., 1., 2., 2., 6., 1., 1., 1., 1., 1.])]\n"
     ]
    }
   ],
   "source": [
    "# Debugging bag of words\n",
    "out = getBagOfWords(Xval,ypred,Yval,words)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method  validation for single DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying Random Forest of weakly trained Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 666us/step - loss: 0.4852 - acc: 0.7666\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 167us/step - loss: 0.3227 - acc: 0.8596\n",
      "16000/16000 [==============================] - 6s 394us/step\n",
      "4000/4000 [==============================] - 1s 220us/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 9s 546us/step - loss: 0.4764 - acc: 0.7746\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.3151 - acc: 0.8611\n",
      "16000/16000 [==============================] - 7s 416us/step\n",
      "4000/4000 [==============================] - 1s 293us/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 661us/step - loss: 0.4841 - acc: 0.7692\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 168us/step - loss: 0.3177 - acc: 0.8638\n",
      "16000/16000 [==============================] - 6s 383us/step\n",
      "4000/4000 [==============================] - 1s 244us/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 9s 593us/step - loss: 0.4864 - acc: 0.7689\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3198 - acc: 0.8623\n",
      "16000/16000 [==============================] - 6s 390us/step\n",
      "4000/4000 [==============================] - 1s 227us/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 9s 593us/step - loss: 0.4747 - acc: 0.7714\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 169us/step - loss: 0.3197 - acc: 0.8619\n",
      "16000/16000 [==============================] - 6s 393us/step\n",
      "4000/4000 [==============================] - 1s 226us/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.4896 - acc: 0.767 - 10s 615us/step - loss: 0.4895 - acc: 0.7676\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.3176 - acc: 0.8626\n",
      "16000/16000 [==============================] - 6s 403us/step\n",
      "4000/4000 [==============================] - 1s 256us/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 612us/step - loss: 0.4723 - acc: 0.7731\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 187us/step - loss: 0.3180 - acc: 0.8626\n",
      "16000/16000 [==============================] - 7s 446us/step\n",
      "4000/4000 [==============================] - 1s 338us/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 728us/step - loss: 0.4767 - acc: 0.7732\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 4s 229us/step - loss: 0.3156 - acc: 0.8632\n",
      "16000/16000 [==============================] - 7s 455us/step\n",
      "4000/4000 [==============================] - 1s 189us/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 621us/step - loss: 0.4685 - acc: 0.7750\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 152us/step - loss: 0.3111 - acc: 0.8644\n",
      "16000/16000 [==============================] - 7s 441us/step\n",
      "4000/4000 [==============================] - 1s 235us/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 620us/step - loss: 0.4769 - acc: 0.7721\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 178us/step - loss: 0.3161 - acc: 0.8631\n",
      "16000/16000 [==============================] - 7s 429us/step\n",
      "4000/4000 [==============================] - 1s 258us/step\n",
      "Training DNN  10\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 661us/step - loss: 0.4855 - acc: 0.7702\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 201us/step - loss: 0.3193 - acc: 0.8589\n",
      "16000/16000 [==============================] - 7s 440us/step\n",
      "4000/4000 [==============================] - 1s 242us/step\n",
      "Training DNN  11\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 649us/step - loss: 0.4834 - acc: 0.7641\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 184us/step - loss: 0.3174 - acc: 0.8627\n",
      "16000/16000 [==============================] - 7s 428us/step\n",
      "4000/4000 [==============================] - 1s 235us/step\n",
      "Training DNN  12\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 631us/step - loss: 0.4754 - acc: 0.7717\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 172us/step - loss: 0.3227 - acc: 0.8598\n",
      "16000/16000 [==============================] - 7s 410us/step\n",
      "4000/4000 [==============================] - 1s 224us/step\n",
      "Training DNN  13\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 669us/step - loss: 0.4856 - acc: 0.7651\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 162us/step - loss: 0.3177 - acc: 0.8614\n",
      "16000/16000 [==============================] - 7s 446us/step\n",
      "4000/4000 [==============================] - 1s 253us/step\n",
      "Training DNN  14\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 662us/step - loss: 0.4895 - acc: 0.7658\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 183us/step - loss: 0.3224 - acc: 0.8623\n",
      "16000/16000 [==============================] - 7s 433us/step\n",
      "4000/4000 [==============================] - 1s 240us/step\n",
      "Training DNN  15\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 638us/step - loss: 0.4899 - acc: 0.7672\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 171us/step - loss: 0.3198 - acc: 0.8618\n",
      "16000/16000 [==============================] - 7s 433us/step\n",
      "4000/4000 [==============================] - 1s 269us/step\n",
      "Training DNN  16\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 644us/step - loss: 0.4828 - acc: 0.7681\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3196 - acc: 0.8669\n",
      "16000/16000 [==============================] - 7s 440us/step\n",
      "4000/4000 [==============================] - 1s 246us/step\n",
      "Training DNN  17\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 687us/step - loss: 0.4901 - acc: 0.7662\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 187us/step - loss: 0.3262 - acc: 0.8556\n",
      "16000/16000 [==============================] - 7s 426us/step\n",
      "4000/4000 [==============================] - 1s 242us/step\n",
      "Training DNN  18\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 666us/step - loss: 0.4884 - acc: 0.7659\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 186us/step - loss: 0.3207 - acc: 0.8618\n",
      "16000/16000 [==============================] - 7s 450us/step\n",
      "4000/4000 [==============================] - 1s 237us/step\n",
      "Training DNN  19\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 696us/step - loss: 0.4841 - acc: 0.7682\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 171us/step - loss: 0.3198 - acc: 0.8622\n",
      "16000/16000 [==============================] - 7s 463us/step\n",
      "4000/4000 [==============================] - 1s 248us/step\n",
      "Training DNN  20\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 762us/step - loss: 0.4826 - acc: 0.7692\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 169us/step - loss: 0.3122 - acc: 0.8665\n",
      "16000/16000 [==============================] - 7s 446us/step\n",
      "4000/4000 [==============================] - 1s 247us/step\n",
      "Training DNN  21\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 678us/step - loss: 0.4832 - acc: 0.7712\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 175us/step - loss: 0.3167 - acc: 0.8635\n",
      "16000/16000 [==============================] - 7s 460us/step\n",
      "4000/4000 [==============================] - 1s 278us/step\n",
      "Training DNN  22\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 694us/step - loss: 0.4912 - acc: 0.7621\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 176us/step - loss: 0.3186 - acc: 0.8603\n",
      "16000/16000 [==============================] - 7s 449us/step\n",
      "4000/4000 [==============================] - 1s 253us/step\n",
      "Training DNN  23\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 693us/step - loss: 0.4958 - acc: 0.7643\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 173us/step - loss: 0.3202 - acc: 0.8644\n",
      "16000/16000 [==============================] - 8s 471us/step\n",
      "4000/4000 [==============================] - 1s 252us/step\n",
      "Training DNN  24\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 11s 673us/step - loss: 0.4900 - acc: 0.7640\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 178us/step - loss: 0.3209 - acc: 0.8605\n",
      "16000/16000 [==============================] - 7s 463us/step\n",
      "4000/4000 [==============================] - 1s 239us/step\n",
      "Training DNN  25\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 718us/step - loss: 0.4798 - acc: 0.7744\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 196us/step - loss: 0.3185 - acc: 0.8611\n",
      "16000/16000 [==============================] - 8s 479us/step\n",
      "4000/4000 [==============================] - 1s 297us/step\n",
      "Training DNN  26\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 695us/step - loss: 0.4902 - acc: 0.7641\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.3243 - acc: 0.8619\n",
      "16000/16000 [==============================] - 8s 492us/step\n",
      "4000/4000 [==============================] - 1s 246us/step\n",
      "Training DNN  27\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 777us/step - loss: 0.4869 - acc: 0.7678\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 210us/step - loss: 0.3176 - acc: 0.8639\n",
      "16000/16000 [==============================] - 9s 567us/step\n",
      "4000/4000 [==============================] - 1s 245us/step\n",
      "Training DNN  28\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 718us/step - loss: 0.4852 - acc: 0.7682\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 191us/step - loss: 0.3191 - acc: 0.8599\n",
      "16000/16000 [==============================] - 8s 517us/step\n",
      "4000/4000 [==============================] - 1s 347us/step\n",
      "Training DNN  29\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 750us/step - loss: 0.4803 - acc: 0.7707\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 170us/step - loss: 0.3199 - acc: 0.8612\n",
      "16000/16000 [==============================] - 8s 494us/step\n",
      "4000/4000 [==============================] - 1s 287us/step\n",
      "Training DNN  30\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 752us/step - loss: 0.4809 - acc: 0.7706\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 182us/step - loss: 0.3197 - acc: 0.8618\n",
      "16000/16000 [==============================] - 8s 497us/step\n",
      "4000/4000 [==============================] - 1s 254us/step\n",
      "Training DNN  31\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 742us/step - loss: 0.4743 - acc: 0.7710\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 190us/step - loss: 0.3166 - acc: 0.8619\n",
      "16000/16000 [==============================] - 8s 477us/step\n",
      "4000/4000 [==============================] - 1s 263us/step\n",
      "Training DNN  32\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 13s 803us/step - loss: 0.4816 - acc: 0.7682\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 205us/step - loss: 0.3209 - acc: 0.8622\n",
      "16000/16000 [==============================] - 8s 517us/step\n",
      "4000/4000 [==============================] - 1s 280us/step\n",
      "Training DNN  33\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 867us/step - loss: 0.4830 - acc: 0.7655\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3161 - acc: 0.8641\n",
      "16000/16000 [==============================] - 9s 573us/step\n",
      "4000/4000 [==============================] - 1s 252us/step\n",
      "Training DNN  34\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 13s 821us/step - loss: 0.4767 - acc: 0.7731\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 4s 220us/step - loss: 0.3202 - acc: 0.8609\n",
      "16000/16000 [==============================] - 9s 562us/step\n",
      "4000/4000 [==============================] - 1s 274us/step\n",
      "Training DNN  35\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 13s 794us/step - loss: 0.4815 - acc: 0.7721\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 4s 227us/step - loss: 0.3183 - acc: 0.8616\n",
      "16000/16000 [==============================] - 11s 682us/step\n",
      "4000/4000 [==============================] - 1s 267us/step\n",
      "Training DNN  36\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 895us/step - loss: 0.4831 - acc: 0.7714\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 4s 262us/step - loss: 0.3167 - acc: 0.8641\n",
      "16000/16000 [==============================] - 8s 524us/step\n",
      "4000/4000 [==============================] - 1s 221us/step\n",
      "Training DNN  37\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 735us/step - loss: 0.4788 - acc: 0.7746\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 4s 257us/step - loss: 0.3210 - acc: 0.8609\n",
      "16000/16000 [==============================] - 9s 588us/step\n",
      "4000/4000 [==============================] - 1s 308us/step\n",
      "Training DNN  38\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 722us/step - loss: 0.4825 - acc: 0.7681\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3211 - acc: 0.8593\n",
      "16000/16000 [==============================] - 8s 483us/step\n",
      "4000/4000 [==============================] - 1s 255us/step\n",
      "Training DNN  39\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 729us/step - loss: 0.4911 - acc: 0.7641\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.3212 - acc: 0.8612\n",
      "16000/16000 [==============================] - 8s 487us/step\n",
      "4000/4000 [==============================] - 1s 256us/step\n",
      "Training DNN  40\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 731us/step - loss: 0.4744 - acc: 0.7715\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3133 - acc: 0.8641\n",
      "16000/16000 [==============================] - 8s 493us/step\n",
      "4000/4000 [==============================] - 1s 264us/step\n",
      "Training DNN  41\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 736us/step - loss: 0.4775 - acc: 0.7698\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3154 - acc: 0.8636\n",
      "16000/16000 [==============================] - 8s 497us/step\n",
      "4000/4000 [==============================] - 1s 266us/step\n",
      "Training DNN  42\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 748us/step - loss: 0.4852 - acc: 0.7638\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.3220 - acc: 0.8586\n",
      "16000/16000 [==============================] - 8s 498us/step\n",
      "4000/4000 [==============================] - 1s 264us/step\n",
      "Training DNN  43\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 744us/step - loss: 0.4848 - acc: 0.7691\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3223 - acc: 0.8594\n",
      "16000/16000 [==============================] - 8s 494us/step\n",
      "4000/4000 [==============================] - 1s 261us/step\n",
      "Training DNN  44\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 748us/step - loss: 0.5065 - acc: 0.7664\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3191 - acc: 0.8630\n",
      "16000/16000 [==============================] - 8s 501us/step\n",
      "4000/4000 [==============================] - 1s 258us/step\n",
      "Training DNN  45\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 754us/step - loss: 0.4810 - acc: 0.7726\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3189 - acc: 0.8629\n",
      "16000/16000 [==============================] - 8s 501us/step\n",
      "4000/4000 [==============================] - 1s 278us/step\n",
      "Training DNN  46\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 768us/step - loss: 0.4928 - acc: 0.7638\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3191 - acc: 0.8629\n",
      "16000/16000 [==============================] - 8s 503us/step\n",
      "4000/4000 [==============================] - 1s 263us/step\n",
      "Training DNN  47\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 765us/step - loss: 0.4806 - acc: 0.7702\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 166us/step - loss: 0.3119 - acc: 0.8677\n",
      "16000/16000 [==============================] - 8s 508us/step\n",
      "4000/4000 [==============================] - 1s 265us/step\n",
      "Training DNN  48\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 12s 769us/step - loss: 0.4757 - acc: 0.7714\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 148us/step - loss: 0.3113 - acc: 0.8654\n",
      "16000/16000 [==============================] - 7s 459us/step\n",
      "4000/4000 [==============================] - 1s 212us/step\n",
      "Training DNN  49\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 752us/step - loss: 0.4843 - acc: 0.7681\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 145us/step - loss: 0.3182 - acc: 0.8632\n",
      "16000/16000 [==============================] - 8s 477us/step\n",
      "4000/4000 [==============================] - 1s 214us/step\n"
     ]
    }
   ],
   "source": [
    "# Split the training data k-fold number of ways for k-fold validation of the learning algorithm\n",
    "k=5\n",
    "kf = sklearn.model_selection.KFold(n_splits=k)\n",
    "inds = [ind for ind in kf.split(x_tall, y_tall)]\n",
    "\n",
    "train,val = inds[0]\n",
    "# Training and validation data for k fold cross validation\n",
    "Xtrain = x_tall[train]\n",
    "Ytrain = y_tall[train]\n",
    "Xval = x_tall[val]\n",
    "Yval = y_tall[val]\n",
    "\n",
    "# Specify number of Neural networks to train\n",
    "N_models = 50\n",
    "Predictions = []\n",
    "TrainErr = []\n",
    "TestErr = []\n",
    "\n",
    "# Define the DNN model\n",
    "for i in range(0,N_models):\n",
    "    print('Training DNN ',i)\n",
    "\n",
    "    # Generate model and fit to data\n",
    "    model = getModel([500,250,125],0.4)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "    model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=2)\n",
    "\n",
    "    # Store the models\n",
    "    TrainErr.append(model.evaluate(x=Xtrain, y=Ytrain))\n",
    "    TestErr.append(model.evaluate(x=Xval, y=Yval))\n",
    "    # Use weakly trained model to predict and store predictions\n",
    "    ypred = model.predict(Xval,batch_size=2**8)\n",
    "    Predictions.append(ypred)\n",
    "\n",
    "Predictions = np.array(Predictions)\n",
    "TrainErr = np.array(TrainErr)\n",
    "TestErr = np.array(TestErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples where stdev > 0,  1382\n",
      "Final accuracy of random forest =  0.863\n"
     ]
    }
   ],
   "source": [
    "ypred = []\n",
    "for i in Predictions:\n",
    "    ypred.append(Unencode(i).astype(int))\n",
    "\n",
    "# Get mean and standard deviation of samples\n",
    "ypmean=np.mean(ypred,axis=0)\n",
    "std=np.std(ypred,axis=0)\n",
    "\n",
    "print('Number of samples where stdev > 0, ',np.sum(std>0))\n",
    "ypred = (ypmean > 0.5).astype(int)\n",
    "ytrue = Unencode(Yval).astype(int)\n",
    "\n",
    "acc = 1.0*np.sum(ypred == ytrue)/len(ytrue)\n",
    "print('Final accuracy of random forest = ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21ec7f0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to get explicit ensemble model accuracy from softmax\n",
    "def getEnsembleAccuracy(Predictions,Yval):\n",
    "    ypred = []\n",
    "    # undo one hot encoding\n",
    "    for i in Predictions:\n",
    "        ypred.append(Unencode(i).astype(int))\n",
    "    # Average the predictions and get final predictions\n",
    "    ypmean=np.mean(ypred,axis=0)\n",
    "    ypred = (ypmean > 0.5).astype(int)\n",
    "    \n",
    "    # Get explicit accuracy\n",
    "    ytrue = Unencode(Yval).astype(int)\n",
    "    acc = 1.0*np.sum(ypred == ytrue)/len(ytrue)\n",
    "    return acc\n",
    "\n",
    "Predictions = np.array(Predictions)\n",
    "acc = []\n",
    "for i in range(1,len(Predictions)+1):\n",
    "    acc.append([np.mean(TrainErr[:i,1]),np.mean(TestErr[:i,1]),getEnsembleAccuracy(Predictions[:i,:],Yval)])\n",
    "\n",
    "acc = np.array(acc)\n",
    "    \n",
    "plt.figure(2,figsize=(5,5))\n",
    "plt.title('Bagging the neural networks')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('N ensemble Neural Networks')\n",
    "plt.plot(np.arange(1,51),acc[:,0],label='Avg training accuracy of individual neuron',color='red')\n",
    "plt.plot(np.arange(1,51),acc[:,1],label='Avg validation accuracy of individual neuron',color='blue')\n",
    "plt.plot(np.arange(1,51),acc[:,2],label='Ensemble validation accuracy',color='green')\n",
    "plt.legend()\n",
    "plt.savefig('NNbag.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random neuron forest\n",
    "def getDNNForestCross(k,N_models,x_tall,y_tall):\n",
    "    # Storage for k fold cross validation\n",
    "    facc = []\n",
    "    \n",
    "    # Split the training data k-fold number of ways for k-fold validation of the learning algorithm\n",
    "    k=5\n",
    "    kf = sklearn.model_selection.KFold(n_splits=k)\n",
    "    inds = [ind for ind in kf.split(x_tall, y_tall)]\n",
    "\n",
    "    j=0\n",
    "    for train,val in inds:\n",
    "        print('Cross fold validation ',j+1,'/',k)\n",
    "        # Training and validation data for k fold cross validation\n",
    "        Xtrain = x_tall[train]\n",
    "        Ytrain = y_tall[train]\n",
    "        Xval = x_tall[val]\n",
    "        Yval = y_tall[val]\n",
    "\n",
    "        # Store predictions from each tree in DNN forest\n",
    "        Predictions = []\n",
    "        \n",
    "        # Define the DNN model\n",
    "        for i in range(0,N_models):\n",
    "            print('Training DNN ',i)\n",
    "            model = getModel([500,250,125],0)\n",
    "            # Compile it and fit\n",
    "            model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "            model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=2,verbose=1)\n",
    "\n",
    "            # Use weakly trained model to predict and store predictions\n",
    "            ypred = model.predict(Xval,batch_size=2**8,verbose=1)\n",
    "            Predictions.append(ypred)\n",
    "\n",
    "        Predictions = np.array(Predictions)\n",
    "        \n",
    "        ypred = []\n",
    "        for i in Predictions:\n",
    "            ypred.append(Unencode(i).astype(int))\n",
    "\n",
    "        # Get mean and standard deviation of samples\n",
    "        ypmean=np.mean(ypred,axis=0)\n",
    "        std=np.std(ypred,axis=0)\n",
    "        print('Number of samples where stdev > 0, ',np.sum(std>0))\n",
    "        \n",
    "        # Compute accuracy of predictions\n",
    "        ypred = (ypmean > 0.5).astype(int)\n",
    "        ytrue = Unencode(Yval).astype(int)\n",
    "        acc = 1.0*np.sum(ypred == ytrue)/len(ytrue)\n",
    "        print('Final accuracy of random forest = ',acc)\n",
    "        j=j+1\n",
    "        facc.append(acc)\n",
    "        \n",
    "    print('avg Accuracy of all random forests',np.mean(facc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross fold validation  0 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 13s 797us/step - loss: 0.4342 - acc: 0.8000\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 154us/step - loss: 0.1825 - acc: 0.9311\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 844us/step - loss: 0.4393 - acc: 0.7969\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 181us/step - loss: 0.1899 - acc: 0.9297\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 13s 813us/step - loss: 0.4315 - acc: 0.8037\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 197us/step - loss: 0.1785 - acc: 0.9337\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 849us/step - loss: 0.4482 - acc: 0.7908\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 161us/step - loss: 0.1801 - acc: 0.9327\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 914us/step - loss: 0.4264 - acc: 0.8039\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 167us/step - loss: 0.1872 - acc: 0.9296\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 868us/step - loss: 0.4354 - acc: 0.8004\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.1803 - acc: 0.9325\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 901us/step - loss: 0.4466 - acc: 0.7948\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1833 - acc: 0.9324 1s - los\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 877us/step - loss: 0.4276 - acc: 0.8001\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 193us/step - loss: 0.1797 - acc: 0.9323\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 926us/step - loss: 0.4320 - acc: 0.8007\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 168us/step - loss: 0.1847 - acc: 0.9292\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 918us/step - loss: 0.4365 - acc: 0.7991\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.1814 - acc: 0.9316\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Number of samples where stdev > 0,  1412\n",
      "Final accuracy of random forest =  0.85875\n",
      "Cross fold validation  1 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 953us/step - loss: 0.4319 - acc: 0.8009\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 170us/step - loss: 0.1727 - acc: 0.9356\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 895us/step - loss: 0.4277 - acc: 0.8003\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 155us/step - loss: 0.1778 - acc: 0.9345\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 956us/step - loss: 0.4280 - acc: 0.7999\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 178us/step - loss: 0.1737 - acc: 0.9371\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 974us/step - loss: 0.4380 - acc: 0.7984\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 179us/step - loss: 0.1732 - acc: 0.9376\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 926us/step - loss: 0.4274 - acc: 0.8024\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 174us/step - loss: 0.1778 - acc: 0.9344\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 942us/step - loss: 0.4268 - acc: 0.8018\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 181us/step - loss: 0.1781 - acc: 0.9345\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 954us/step - loss: 0.4271 - acc: 0.8026\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 183us/step - loss: 0.1814 - acc: 0.9299\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 922us/step - loss: 0.4383 - acc: 0.7955\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 157us/step - loss: 0.1805 - acc: 0.9323\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 985us/step - loss: 0.4259 - acc: 0.8011\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 203us/step - loss: 0.1843 - acc: 0.9308\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 933us/step - loss: 0.4337 - acc: 0.8018\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 214us/step - loss: 0.1791 - acc: 0.9334\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Number of samples where stdev > 0,  1477\n",
      "Final accuracy of random forest =  0.8565\n",
      "Cross fold validation  2 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 943us/step - loss: 0.4328 - acc: 0.7986\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 176us/step - loss: 0.1779 - acc: 0.9344\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 916us/step - loss: 0.4273 - acc: 0.8040\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 172us/step - loss: 0.1764 - acc: 0.9368\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 926us/step - loss: 0.4320 - acc: 0.8034\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 174us/step - loss: 0.1699 - acc: 0.9351\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4280 - acc: 0.8054\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 215us/step - loss: 0.1779 - acc: 0.9364\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 949us/step - loss: 0.4283 - acc: 0.8006\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1789 - acc: 0.9339\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 999us/step - loss: 0.4344 - acc: 0.7976\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.1716 - acc: 0.9378\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 934us/step - loss: 0.4269 - acc: 0.8023\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1805 - acc: 0.9324\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 940us/step - loss: 0.4355 - acc: 0.7994\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 170us/step - loss: 0.1769 - acc: 0.9353\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4364 - acc: 0.8008\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 194us/step - loss: 0.1781 - acc: 0.9326\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4347 - acc: 0.7999\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 158us/step - loss: 0.1776 - acc: 0.9346\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Number of samples where stdev > 0,  1377\n",
      "Final accuracy of random forest =  0.84375\n",
      "Cross fold validation  3 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 979us/step - loss: 0.4330 - acc: 0.7994\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 177us/step - loss: 0.1770 - acc: 0.9359\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 948us/step - loss: 0.4278 - acc: 0.8039\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 143us/step - loss: 0.1769 - acc: 0.9340\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4365 - acc: 0.7973\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 167us/step - loss: 0.1760 - acc: 0.9360\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4252 - acc: 0.8042\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.1731 - acc: 0.9356\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 966us/step - loss: 0.4374 - acc: 0.7981\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 182us/step - loss: 0.1804 - acc: 0.9327\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4335 - acc: 0.7999\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 151us/step - loss: 0.1743 - acc: 0.9341\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4247 - acc: 0.8025\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 5s 301us/step - loss: 0.1781 - acc: 0.9342\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4332 - acc: 0.8029\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 4s 241us/step - loss: 0.1710 - acc: 0.9393\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4344 - acc: 0.8004\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 193us/step - loss: 0.1707 - acc: 0.9379 1s - loss: \n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4299 - acc: 0.8013\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 174us/step - loss: 0.1760 - acc: 0.9359\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Number of samples where stdev > 0,  1400\n",
      "Final accuracy of random forest =  0.84675\n",
      "Cross fold validation  4 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4308 - acc: 0.7984\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 167us/step - loss: 0.1777 - acc: 0.9331\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 999us/step - loss: 0.4294 - acc: 0.8018\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.1806 - acc: 0.9347\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4283 - acc: 0.8021\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.1834 - acc: 0.9310\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4428 - acc: 0.7974\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.1747 - acc: 0.9364\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4416 - acc: 0.7973\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 157us/step - loss: 0.1758 - acc: 0.9333\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4360 - acc: 0.8007\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 170us/step - loss: 0.1778 - acc: 0.9332\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4361 - acc: 0.7977\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 178us/step - loss: 0.1772 - acc: 0.9343\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4423 - acc: 0.7996\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 181us/step - loss: 0.1736 - acc: 0.9357\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4370 - acc: 0.7953\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 161us/step - loss: 0.1745 - acc: 0.9354\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4249 - acc: 0.8009\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.1794 - acc: 0.9340\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Number of samples where stdev > 0,  1661\n",
      "Final accuracy of random forest =  0.8485\n",
      "avg Accuracy of all random forests 0.8508500000000001\n"
     ]
    }
   ],
   "source": [
    "getDNNForestCross(5,10,x_tall,y_tall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output final predictions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify number of Neural networks to train\n",
    "N_models = 20\n",
    "Predictions = []\n",
    "\n",
    "# Define the DNN model\n",
    "for i in range(0,N_models):\n",
    "    print('Training DNN ',i)\n",
    "    model = getModel([500,250,125],0.4)\n",
    "    # Compile it and fit\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "    model.fit(x_tall, y_tall, batch_size=2**8, epochs=2)\n",
    "    # Use weakly trained model to predict and store predictions\n",
    "    ypred = model.predict(FV_data,batch_size=2**8)\n",
    "    Predictions.append(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions = np.array(Predictions)\n",
    "\n",
    "ypred = []\n",
    "for i in Predictions:\n",
    "    ypred.append(Unencode(i).astype(int))\n",
    "\n",
    "# Get mean and standard deviation of samples\n",
    "ypmean=np.mean(ypred,axis=0)\n",
    "std=np.std(ypred,axis=0)\n",
    "print('Number of samples where stdev > 0, ',np.sum(std>0))\n",
    "\n",
    "# Compute final predictions and output it\n",
    "ypred = (ypmean > 0.5).astype(int)\n",
    "writeResults(ypred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
