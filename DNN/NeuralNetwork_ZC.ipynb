{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS155 Miniproject 1\n",
    "\n",
    "zchen@caltech.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis via Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final validation data set\n",
    "FV_data = np.loadtxt('../data/test_data.txt',delimiter=' ',skiprows=1)\n",
    "# Load input training data set\n",
    "train_data = np.loadtxt('../data/training_data.txt',delimiter=' ',skiprows=1)\n",
    "\n",
    "# Get header words list\n",
    "f = open('../data/test_data.txt','r')\n",
    "words = np.array(f.readline().split())\n",
    "f.close()\n",
    "\n",
    "# Splot y_train and x_train from training set\n",
    "x_tall = train_data[:,1:]\n",
    "y_tall = train_data[:,0]\n",
    "\n",
    "# One hot encode categories\n",
    "y_tall = keras.utils.np_utils.to_categorical(y_tall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Neural Networks on bag of words dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate DNN of given depth and width\n",
    "def getModel(layers,Pdrop):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layers[0],input_shape=(1000,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(Pdrop))\n",
    "    for i in layers[1:]:\n",
    "        model.add(Dense(i))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "    # predicting probabilities of each of the 2 classes\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the DNN model\n",
    "def trainModels(k,x_tall,y_tall):\n",
    "    # Storage for k fold cross validation    \n",
    "    trainErr = []\n",
    "    testErr = []\n",
    "    acc = []\n",
    "    \n",
    "    # Split the training data k-fold number of ways for k-fold validation of the learning algorithm\n",
    "    kf = sklearn.model_selection.KFold(n_splits=k)\n",
    "    inds = [ind for ind in kf.split(x_tall, y_tall)]\n",
    "    \n",
    "    i=0\n",
    "    for train,val in inds:\n",
    "        # Training and validation data for k fold cross validation\n",
    "        Xtrain = x_tall[train]\n",
    "        Ytrain = y_tall[train]\n",
    "        Xval = x_tall[val]\n",
    "        Yval = y_tall[val]\n",
    "    \n",
    "        # Define the DNN model\n",
    "        model = getModel([500,250,125],0.4)\n",
    "        \n",
    "        # Compile it and fit\n",
    "        model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "        model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=10,verbose=1,validation_data=(Xval, Yval))\n",
    "        \n",
    "        # store training and test error\n",
    "        trainErr.append(model.evaluate(x=Xtrain, y=Ytrain))\n",
    "        testErr.append(model.evaluate(x=Xval, y=Yval))\n",
    "        acc.append(getAccuracy(model,Xval,Yval))\n",
    "\n",
    "        # Status output\n",
    "        print('k-iteration = ',i)\n",
    "        i=i+1\n",
    "        \n",
    "    return np.array(trainErr),np.array(testErr),np.array(acc)\n",
    "\n",
    "# undo one hot encoding\n",
    "def Unencode(out):\n",
    "    ypred = out[:,0] < out[:,1]\n",
    "    return ypred\n",
    "\n",
    "# Function to get explicit model accuracy from softmax\n",
    "def getAccuracy(model,xt,yt):\n",
    "    out = model.predict(xt)\n",
    "    ypred = Unencode(out)\n",
    "    ytrue = Unencode(yt)\n",
    "    acc = 1.0*np.sum(ypred == ytrue)/len(ytrue)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 6s 399us/step - loss: 0.4819 - acc: 0.7697 - val_loss: 0.3581 - val_acc: 0.8390\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.3154 - acc: 0.8621 - val_loss: 0.3339 - val_acc: 0.8655\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.2547 - acc: 0.8930 - val_loss: 0.3446 - val_acc: 0.8582\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 2s 149us/step - loss: 0.2089 - acc: 0.9137 - val_loss: 0.3923 - val_acc: 0.8548\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.1729 - acc: 0.9291 - val_loss: 0.4131 - val_acc: 0.8560\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.1319 - acc: 0.9495 - val_loss: 0.4641 - val_acc: 0.8505\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.1084 - acc: 0.9585 - val_loss: 0.4971 - val_acc: 0.8545\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.0903 - acc: 0.9655 - val_loss: 0.5307 - val_acc: 0.8498\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.0718 - acc: 0.9744 - val_loss: 0.6195 - val_acc: 0.8435\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.0611 - acc: 0.9772 - val_loss: 0.5946 - val_acc: 0.8470\n",
      "16000/16000 [==============================] - 2s 130us/step\n",
      "4000/4000 [==============================] - 1s 130us/step\n",
      "k-iteration =  0\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 8s 478us/step - loss: 0.4754 - acc: 0.7738 - val_loss: 0.3732 - val_acc: 0.8332\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.3198 - acc: 0.8614 - val_loss: 0.3608 - val_acc: 0.8495\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.2541 - acc: 0.8924 - val_loss: 0.3769 - val_acc: 0.8480\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.2114 - acc: 0.9148 - val_loss: 0.4009 - val_acc: 0.8450\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.1672 - acc: 0.9347 - val_loss: 0.4682 - val_acc: 0.8420\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.1347 - acc: 0.9467 - val_loss: 0.4861 - val_acc: 0.8345\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.1103 - acc: 0.9576 - val_loss: 0.5442 - val_acc: 0.8333\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.0860 - acc: 0.9674 - val_loss: 0.5995 - val_acc: 0.8335\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.0764 - acc: 0.9722 - val_loss: 0.6548 - val_acc: 0.8307\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.0600 - acc: 0.9784 - val_loss: 0.5893 - val_acc: 0.8465\n",
      "16000/16000 [==============================] - 2s 134us/step\n",
      "4000/4000 [==============================] - 1s 132us/step\n",
      "k-iteration =  1\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 7s 410us/step - loss: 0.4828 - acc: 0.7706 - val_loss: 0.3798 - val_acc: 0.8285\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.3142 - acc: 0.8654 - val_loss: 0.3763 - val_acc: 0.8357\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.2515 - acc: 0.8964 - val_loss: 0.4323 - val_acc: 0.8357\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.2069 - acc: 0.9173 - val_loss: 0.4108 - val_acc: 0.8462\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.1638 - acc: 0.9359 - val_loss: 0.4913 - val_acc: 0.8365\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.1341 - acc: 0.9486 - val_loss: 0.4752 - val_acc: 0.8480\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.1047 - acc: 0.9615 - val_loss: 0.5809 - val_acc: 0.8308\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.0846 - acc: 0.9683 - val_loss: 0.5894 - val_acc: 0.8427\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.0727 - acc: 0.9724 - val_loss: 0.6738 - val_acc: 0.8347\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.0631 - acc: 0.9763 - val_loss: 0.6747 - val_acc: 0.8355\n",
      "16000/16000 [==============================] - 2s 139us/step\n",
      "4000/4000 [==============================] - 1s 139us/step\n",
      "k-iteration =  2\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 7s 413us/step - loss: 0.4726 - acc: 0.7737 - val_loss: 0.3790 - val_acc: 0.8302\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 2s 148us/step - loss: 0.3155 - acc: 0.8641 - val_loss: 0.3696 - val_acc: 0.8452\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.2443 - acc: 0.8985 - val_loss: 0.3857 - val_acc: 0.8422\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.2000 - acc: 0.9217 - val_loss: 0.4420 - val_acc: 0.8405\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.1604 - acc: 0.9355 - val_loss: 0.4732 - val_acc: 0.8403\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.1314 - acc: 0.9497 - val_loss: 0.5126 - val_acc: 0.8375\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 2s 148us/step - loss: 0.1086 - acc: 0.9583 - val_loss: 0.5350 - val_acc: 0.8420\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.0893 - acc: 0.9654 - val_loss: 0.6300 - val_acc: 0.8280\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.0736 - acc: 0.9733 - val_loss: 0.6447 - val_acc: 0.8390\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 2s 148us/step - loss: 0.0606 - acc: 0.9769 - val_loss: 0.6968 - val_acc: 0.8402\n",
      "16000/16000 [==============================] - 2s 133us/step\n",
      "4000/4000 [==============================] - 1s 134us/step\n",
      "k-iteration =  3\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 7s 418us/step - loss: 0.4766 - acc: 0.7706 - val_loss: 0.3809 - val_acc: 0.8315\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.3105 - acc: 0.8656 - val_loss: 0.3651 - val_acc: 0.8418\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.2540 - acc: 0.8926 - val_loss: 0.3918 - val_acc: 0.8345\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.2114 - acc: 0.9119 - val_loss: 0.4194 - val_acc: 0.8372\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.1631 - acc: 0.9364 - val_loss: 0.4302 - val_acc: 0.8415\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.1323 - acc: 0.9492 - val_loss: 0.4592 - val_acc: 0.8445\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.1078 - acc: 0.9585 - val_loss: 0.5147 - val_acc: 0.8310\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.0863 - acc: 0.9681 - val_loss: 0.5631 - val_acc: 0.8372\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.0725 - acc: 0.9715 - val_loss: 0.5934 - val_acc: 0.8355\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 2s 147us/step - loss: 0.0625 - acc: 0.9770 - val_loss: 0.6733 - val_acc: 0.8313\n",
      "16000/16000 [==============================] - 2s 137us/step\n",
      "4000/4000 [==============================] - 1s 141us/step\n",
      "k-iteration =  4\n"
     ]
    }
   ],
   "source": [
    "trainErr,testErr,Vacc = trainModels(5,x_tall,y_tall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " trainErr\n",
      "[[0.00951698 0.9980625 ]\n",
      " [0.00941472 0.9986875 ]\n",
      " [0.01647534 0.99575   ]\n",
      " [0.00923579 0.9983125 ]\n",
      " [0.01178948 0.9970625 ]]\n",
      "avg trainErr [0.01128646 0.997575  ]\n",
      "\n",
      " testErr\n",
      "[[0.59455196 0.847     ]\n",
      " [0.58926733 0.8465    ]\n",
      " [0.67467756 0.8355    ]\n",
      " [0.69684073 0.84025   ]\n",
      " [0.67334788 0.83125   ]]\n",
      "avg testErr [0.64573709 0.8401    ]\n",
      "\n",
      " Accuracy\n",
      "[0.847   0.8465  0.8355  0.84025 0.83125]\n",
      "avg acc 0.8401\n"
     ]
    }
   ],
   "source": [
    "print('\\n trainErr')\n",
    "print(trainErr)\n",
    "print('avg trainErr',np.mean(trainErr,axis=0))\n",
    "print('\\n testErr')\n",
    "print(testErr)\n",
    "print('avg testErr',np.mean(testErr,axis=0))\n",
    "print('\\n Accuracy')\n",
    "print(Vacc)\n",
    "print('avg acc',np.mean(Vacc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameters plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy plateaus at 85%\n",
    "\n",
    "Debugging training and test errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/10\n",
      "16000/16000 [==============================] - 6s 365us/step - loss: 0.4876 - acc: 0.7648 - val_loss: 0.3556 - val_acc: 0.8410\n",
      "Epoch 2/10\n",
      "16000/16000 [==============================] - 2s 145us/step - loss: 0.3199 - acc: 0.8604 - val_loss: 0.3461 - val_acc: 0.8520\n",
      "Epoch 3/10\n",
      "16000/16000 [==============================] - 2s 145us/step - loss: 0.2581 - acc: 0.8937 - val_loss: 0.3508 - val_acc: 0.8563\n",
      "Epoch 4/10\n",
      "16000/16000 [==============================] - 2s 145us/step - loss: 0.2101 - acc: 0.9151 - val_loss: 0.3738 - val_acc: 0.8520\n",
      "Epoch 5/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.1709 - acc: 0.9314 - val_loss: 0.4012 - val_acc: 0.8490\n",
      "Epoch 6/10\n",
      "16000/16000 [==============================] - 2s 145us/step - loss: 0.1381 - acc: 0.9449 - val_loss: 0.4880 - val_acc: 0.8360\n",
      "Epoch 7/10\n",
      "16000/16000 [==============================] - 2s 145us/step - loss: 0.1076 - acc: 0.9592 - val_loss: 0.5194 - val_acc: 0.8442\n",
      "Epoch 8/10\n",
      "16000/16000 [==============================] - 2s 145us/step - loss: 0.0933 - acc: 0.9639 - val_loss: 0.5318 - val_acc: 0.8397\n",
      "Epoch 9/10\n",
      "16000/16000 [==============================] - 2s 145us/step - loss: 0.0727 - acc: 0.9734 - val_loss: 0.5772 - val_acc: 0.8390\n",
      "Epoch 10/10\n",
      "16000/16000 [==============================] - 2s 146us/step - loss: 0.0668 - acc: 0.9747 - val_loss: 0.6335 - val_acc: 0.8440\n",
      "4000/4000 [==============================] - 1s 336us/step\n"
     ]
    }
   ],
   "source": [
    "# Split the training data k-fold number of ways for k-fold validation of the learning algorithm\n",
    "k=5\n",
    "kf = sklearn.model_selection.KFold(n_splits=k)\n",
    "inds = [ind for ind in kf.split(x_tall, y_tall)]\n",
    "\n",
    "i=0\n",
    "train,val = inds[0]\n",
    "# Training and validation data for k fold cross validation\n",
    "Xtrain = x_tall[train]\n",
    "Ytrain = y_tall[train]\n",
    "Xval = x_tall[val]\n",
    "Yval = y_tall[val]\n",
    "\n",
    "# Define the DNN model\n",
    "model = getModel([500,250,125],0.4)\n",
    "\n",
    "# Compile it and fit\n",
    "model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=10,verbose=1,validation_data=(Xval, Yval))\n",
    "ypred = model.predict(Xval,batch_size=2**8,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get bag of words which were misclassified\n",
    "def getBagOfWords(xtrain,ypred,ytrue,words):\n",
    "    out = []\n",
    "    ypredu = Unencode(ypred).astype(int)\n",
    "    ytrueu = Unencode(ytrue).astype(int)\n",
    "    # Get locations of bag of words which were misclassified\n",
    "    idx = np.arange(0,len(ypredu))\n",
    "    idxErr = idx[ypredu!=ytrueu]\n",
    "    Xerr = xtrain[ypredu!=ytrueu]\n",
    "    j = 0\n",
    "    for i in Xerr:\n",
    "        out.append([ytrue[idxErr[j]],ypred[idxErr[j]],words[i>0],i[i>0]])\n",
    "        j=j+1\n",
    "    return out\n",
    "\n",
    "out = getBagOfWords(Xval,ypred,Yval,words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 1.]), array([9.993092e-01, 6.907701e-04], dtype=float32), array(['thi', 'veri', 'get', 'onli', 'look', 'want', 'see', 'mani', 'new',\n",
      "       'peopl', 'still', 'need', 'two', 'feel', 'start', 'long', 'listen',\n",
      "       'excel', 'enough', 'person', 'cover', 'almost', 'scene', 'instead',\n",
      "       'famili', 'sever', 'hour', 'els', 'fine', 'talk', 'american',\n",
      "       'entir', 'lack', 'impress', 'state', 'avail', 'certainli',\n",
      "       'student', 'danc', 'parent', 'critic', 'centuri', 'train',\n",
      "       'aspect'], dtype='<U12'), array([3., 2., 1., 1., 2., 1., 2., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       2., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
      "       1., 1., 2., 2., 6., 1., 1., 1., 1., 1.])]\n"
     ]
    }
   ],
   "source": [
    "print(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 7s 368us/step - loss: 0.4787 - acc: 0.7632\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 3s 144us/step - loss: 0.3342 - acc: 0.8569\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.2775 - acc: 0.8807\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 3s 145us/step - loss: 0.2347 - acc: 0.9026\n",
      "10000/10000 [==============================] - 2s 213us/step\n"
     ]
    }
   ],
   "source": [
    "# Function to write final predictions\n",
    "def writeResults(out):\n",
    "    f = open('DNN_submission.txt','w')\n",
    "    f.write('Id,Prediction\\n')\n",
    "    ypred = Unencode(out)\n",
    "    for i in range(0,len(ypred)):\n",
    "        f.write(str(i+1)+','+str(int(ypred[i]))+'\\n')\n",
    "    f.close()\n",
    "\n",
    "# Compile it and fit\n",
    "model = getModel([500,250,125,75,25],0.4)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "fit = model.fit(x_tall, y_tall, batch_size=2**8, epochs=4,verbose=1)\n",
    "out = model.predict(FV_data,batch_size=2**8,verbose=1)\n",
    "\n",
    "writeResults(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
