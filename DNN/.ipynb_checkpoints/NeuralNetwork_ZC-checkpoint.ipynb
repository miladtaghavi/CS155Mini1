{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS155 Miniproject 1\n",
    "\n",
    "zchen@caltech.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis via Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "import sklearn.model_selection\n",
    "\n",
    "import tensorflow as tf \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, LeakyReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final validation data set\n",
    "FV_data = np.loadtxt('../data/test_data.txt',delimiter=' ',skiprows=1)\n",
    "# Load input training data set\n",
    "train_data = np.loadtxt('../data/training_data.txt',delimiter=' ',skiprows=1)\n",
    "utrain_data = np.unique(train_data,axis=0)\n",
    "\n",
    "# Get header words list\n",
    "f = open('../data/test_data.txt','r')\n",
    "words = np.array(f.readline().split())\n",
    "f.close()\n",
    "\n",
    "# Splot y_train and x_train from training set\n",
    "x_tall = train_data[:,1:]\n",
    "y_tall = train_data[:,0]\n",
    "\n",
    "x_uall = utrain_data[:,1:]\n",
    "y_uall = utrain_data[:,0]\n",
    "\n",
    "# One hot encode categories\n",
    "y_tall = keras.utils.np_utils.to_categorical(y_tall)\n",
    "y_uall = keras.utils.np_utils.to_categorical(y_uall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions for Neural network debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate DNN of given depth and width\n",
    "def getModel(layers,Pdrop):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layers[0],input_shape=(1000,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(Pdrop))\n",
    "    for i in layers[1:]:\n",
    "        model.add(Dense(i))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation('relu'))\n",
    "    \n",
    "    # predicting probabilities of each of the 2 classes\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "# undo one hot encoding\n",
    "def Unencode(out):\n",
    "    ypred = out[:,0] < out[:,1]\n",
    "    return ypred\n",
    "\n",
    "# Function to get explicit model accuracy from softmax\n",
    "def getAccuracy(model,xt,yt):\n",
    "    out = model.predict(xt)\n",
    "    ypred = Unencode(out)\n",
    "    ytrue = Unencode(yt)\n",
    "    acc = 1.0*np.sum(ypred == ytrue)/len(ytrue)\n",
    "    return acc\n",
    "\n",
    "# Function to get bag of words which were misclassified\n",
    "def getBagOfWords(xtrain,ypred,ytrue,words):\n",
    "    out = []\n",
    "    ypredu = Unencode(ypred).astype(int)\n",
    "    ytrueu = Unencode(ytrue).astype(int)\n",
    "    # Get locations of bag of words which were misclassified\n",
    "    idx = np.arange(0,len(ypredu))\n",
    "    idxErr = idx[ypredu!=ytrueu]\n",
    "    Xerr = xtrain[ypredu!=ytrueu]\n",
    "    j = 0\n",
    "    for i in Xerr:\n",
    "        out.append([ytrue[idxErr[j]],ypred[idxErr[j]],words[i>0],i[i>0]])\n",
    "        j=j+1\n",
    "    return out\n",
    "\n",
    "# Function to write final predictions\n",
    "def writeResults(ypred):\n",
    "    f = open('DNN_submission.txt','w')\n",
    "    f.write('Id,Prediction\\n')\n",
    "    for i in range(0,len(ypred)):\n",
    "        f.write(str(i+1)+','+str(int(ypred[i]))+'\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neural network model for bag of words predictor\n",
    "\n",
    "Initial testing of single DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 4s 236us/step - loss: 0.4844 - acc: 0.7659 - val_loss: 0.3458 - val_acc: 0.8495\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 186us/step - loss: 0.3172 - acc: 0.8629 - val_loss: 0.3455 - val_acc: 0.8562\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 203us/step - loss: 0.2608 - acc: 0.8921 - val_loss: 0.3526 - val_acc: 0.8540\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 204us/step - loss: 0.2123 - acc: 0.9129 - val_loss: 0.3760 - val_acc: 0.8518\n",
      "4000/4000 [==============================] - 0s 102us/step\n"
     ]
    }
   ],
   "source": [
    "# Split the training data k-fold number of ways for k-fold validation of the learning algorithm\n",
    "k=5\n",
    "kf = sklearn.model_selection.KFold(n_splits=k)\n",
    "inds = [ind for ind in kf.split(x_tall, y_tall)]\n",
    "\n",
    "i=0\n",
    "train,val = inds[0]\n",
    "# Training and validation data for k fold cross validation\n",
    "Xtrain = x_tall[train]\n",
    "Ytrain = y_tall[train]\n",
    "Xval = x_tall[val]\n",
    "Yval = y_tall[val]\n",
    "\n",
    "# Define the DNN model\n",
    "model = getModel([500,250,125],0.4)\n",
    "\n",
    "# Compile it and fit\n",
    "model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=4,verbose=1,validation_data=(Xval, Yval))\n",
    "ypred = model.predict(Xval,batch_size=2**8,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 3s 215us/step - loss: 0.4381 - acc: 0.7950 - val_loss: 0.3721 - val_acc: 0.8345\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 2s 152us/step - loss: 0.1811 - acc: 0.9356 - val_loss: 0.4350 - val_acc: 0.8175\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 2s 153us/step - loss: 0.0832 - acc: 0.9754 - val_loss: 0.4603 - val_acc: 0.8315\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 2s 152us/step - loss: 0.0401 - acc: 0.9882 - val_loss: 0.5620 - val_acc: 0.8257\n",
      "16000/16000 [==============================] - 2s 121us/step\n",
      "4000/4000 [==============================] - 1s 129us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 4s 233us/step - loss: 0.4389 - acc: 0.7980 - val_loss: 0.3655 - val_acc: 0.8405\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 170us/step - loss: 0.2024 - acc: 0.9198 - val_loss: 0.3928 - val_acc: 0.8403\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 173us/step - loss: 0.1014 - acc: 0.9679 - val_loss: 0.4513 - val_acc: 0.8360\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 202us/step - loss: 0.0528 - acc: 0.9832 - val_loss: 0.6281 - val_acc: 0.8135\n",
      "16000/16000 [==============================] - 2s 137us/step\n",
      "4000/4000 [==============================] - 1s 138us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 4s 238us/step - loss: 0.4545 - acc: 0.7862 - val_loss: 0.3592 - val_acc: 0.8403\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 174us/step - loss: 0.2280 - acc: 0.9082 - val_loss: 0.3764 - val_acc: 0.8407\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 174us/step - loss: 0.1304 - acc: 0.9524 - val_loss: 0.4364 - val_acc: 0.8422\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 176us/step - loss: 0.0756 - acc: 0.9761 - val_loss: 0.4840 - val_acc: 0.8327\n",
      "16000/16000 [==============================] - 2s 126us/step\n",
      "4000/4000 [==============================] - 1s 133us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 4s 236us/step - loss: 0.4449 - acc: 0.7903 - val_loss: 0.3599 - val_acc: 0.8420\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.2477 - acc: 0.8934 - val_loss: 0.3904 - val_acc: 0.8332\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 162us/step - loss: 0.1566 - acc: 0.9400 - val_loss: 0.4235 - val_acc: 0.8313\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.0956 - acc: 0.9664 - val_loss: 0.5083 - val_acc: 0.8343\n",
      "16000/16000 [==============================] - 2s 125us/step\n",
      "4000/4000 [==============================] - 1s 131us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 4s 238us/step - loss: 0.4637 - acc: 0.7836 - val_loss: 0.3436 - val_acc: 0.8415\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.2633 - acc: 0.8918 - val_loss: 0.4638 - val_acc: 0.8110\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1807 - acc: 0.9297 - val_loss: 0.3766 - val_acc: 0.8470\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1169 - acc: 0.9575 - val_loss: 0.4025 - val_acc: 0.8480\n",
      "16000/16000 [==============================] - 2s 125us/step\n",
      "4000/4000 [==============================] - 1s 125us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 4s 261us/step - loss: 0.4607 - acc: 0.7816 - val_loss: 0.3593 - val_acc: 0.8422\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.2814 - acc: 0.8788 - val_loss: 0.3642 - val_acc: 0.8475\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.1978 - acc: 0.9216 - val_loss: 0.4014 - val_acc: 0.8405\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1432 - acc: 0.9440 - val_loss: 0.4682 - val_acc: 0.8302\n",
      "16000/16000 [==============================] - 2s 129us/step\n",
      "4000/4000 [==============================] - 1s 133us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 4s 252us/step - loss: 0.4640 - acc: 0.7779 - val_loss: 0.3715 - val_acc: 0.8328\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.2976 - acc: 0.8719 - val_loss: 0.3588 - val_acc: 0.8440\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.2230 - acc: 0.9110 - val_loss: 0.3704 - val_acc: 0.8457\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1667 - acc: 0.9351 - val_loss: 0.4454 - val_acc: 0.8330\n",
      "16000/16000 [==============================] - 2s 128us/step\n",
      "4000/4000 [==============================] - 1s 136us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 4s 257us/step - loss: 0.4746 - acc: 0.7685 - val_loss: 0.3552 - val_acc: 0.8482\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3045 - acc: 0.8711 - val_loss: 0.3552 - val_acc: 0.8493\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.2392 - acc: 0.8994 - val_loss: 0.3788 - val_acc: 0.8480\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1876 - acc: 0.9246 - val_loss: 0.3976 - val_acc: 0.8488\n",
      "16000/16000 [==============================] - 2s 134us/step\n",
      "4000/4000 [==============================] - 1s 134us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 4s 261us/step - loss: 0.4830 - acc: 0.7692 - val_loss: 0.3491 - val_acc: 0.8442\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3225 - acc: 0.8587 - val_loss: 0.3395 - val_acc: 0.8522\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.2567 - acc: 0.8910 - val_loss: 0.3779 - val_acc: 0.8427\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 166us/step - loss: 0.2076 - acc: 0.9147 - val_loss: 0.3727 - val_acc: 0.8558\n",
      "16000/16000 [==============================] - 2s 130us/step\n",
      "4000/4000 [==============================] - 1s 133us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 4s 266us/step - loss: 0.5108 - acc: 0.7607 - val_loss: 0.3677 - val_acc: 0.8370\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3298 - acc: 0.8540 - val_loss: 0.3427 - val_acc: 0.8500\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.2724 - acc: 0.8839 - val_loss: 0.3512 - val_acc: 0.8478\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.2270 - acc: 0.9069 - val_loss: 0.3851 - val_acc: 0.8458\n",
      "16000/16000 [==============================] - 2s 131us/step\n",
      "4000/4000 [==============================] - 1s 135us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 4s 272us/step - loss: 0.4967 - acc: 0.7591 - val_loss: 0.3772 - val_acc: 0.8333\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 169us/step - loss: 0.3430 - acc: 0.8473 - val_loss: 0.3396 - val_acc: 0.8522\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 169us/step - loss: 0.2894 - acc: 0.8779 - val_loss: 0.3558 - val_acc: 0.8515\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.2545 - acc: 0.8931 - val_loss: 0.3704 - val_acc: 0.8502\n",
      "16000/16000 [==============================] - 2s 131us/step\n",
      "4000/4000 [==============================] - 1s 132us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 4s 278us/step - loss: 0.5052 - acc: 0.7515 - val_loss: 0.3508 - val_acc: 0.8467\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3515 - acc: 0.8417 - val_loss: 0.3416 - val_acc: 0.8512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 162us/step - loss: 0.3001 - acc: 0.8694 - val_loss: 0.3693 - val_acc: 0.8525\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.2622 - acc: 0.8895 - val_loss: 0.3547 - val_acc: 0.8550\n",
      "16000/16000 [==============================] - 2s 134us/step\n",
      "4000/4000 [==============================] - 1s 132us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 5s 282us/step - loss: 0.5285 - acc: 0.7402 - val_loss: 0.3621 - val_acc: 0.8385\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.3683 - acc: 0.8372 - val_loss: 0.3418 - val_acc: 0.8538\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3167 - acc: 0.8644 - val_loss: 0.3423 - val_acc: 0.8575\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.2883 - acc: 0.8770 - val_loss: 0.3722 - val_acc: 0.8545\n",
      "16000/16000 [==============================] - 2s 136us/step\n",
      "4000/4000 [==============================] - 1s 137us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 5s 316us/step - loss: 0.5346 - acc: 0.7326 - val_loss: 0.3634 - val_acc: 0.8380\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3842 - acc: 0.8281 - val_loss: 0.3454 - val_acc: 0.8500\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3311 - acc: 0.8563 - val_loss: 0.3380 - val_acc: 0.8600\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.2946 - acc: 0.8749 - val_loss: 0.3611 - val_acc: 0.8592\n",
      "16000/16000 [==============================] - 2s 138us/step\n",
      "4000/4000 [==============================] - 1s 142us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 5s 291us/step - loss: 0.5712 - acc: 0.7134 - val_loss: 0.4208 - val_acc: 0.8067\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.4133 - acc: 0.8103 - val_loss: 0.3498 - val_acc: 0.8482\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3447 - acc: 0.8506 - val_loss: 0.3414 - val_acc: 0.8622\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.3202 - acc: 0.8607 - val_loss: 0.3487 - val_acc: 0.8625\n",
      "16000/16000 [==============================] - 2s 133us/step\n",
      "4000/4000 [==============================] - 1s 135us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 5s 295us/step - loss: 0.5757 - acc: 0.7041 - val_loss: 0.3791 - val_acc: 0.8345\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.4155 - acc: 0.8089 - val_loss: 0.3388 - val_acc: 0.8522\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3673 - acc: 0.8372 - val_loss: 0.3388 - val_acc: 0.8585\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3291 - acc: 0.8554 - val_loss: 0.3528 - val_acc: 0.8557\n",
      "16000/16000 [==============================] - 2s 142us/step\n",
      "4000/4000 [==============================] - 1s 144us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 5s 338us/step - loss: 0.6051 - acc: 0.6820 - val_loss: 0.4000 - val_acc: 0.8228\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.4503 - acc: 0.7852 - val_loss: 0.3420 - val_acc: 0.8540\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 166us/step - loss: 0.3882 - acc: 0.8257 - val_loss: 0.3317 - val_acc: 0.8590\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3513 - acc: 0.8495 - val_loss: 0.3528 - val_acc: 0.8613\n",
      "16000/16000 [==============================] - 2s 139us/step\n",
      "4000/4000 [==============================] - 1s 144us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 5s 307us/step - loss: 0.6324 - acc: 0.6555 - val_loss: 0.4217 - val_acc: 0.8092\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.4834 - acc: 0.7722 - val_loss: 0.3436 - val_acc: 0.8478\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 173us/step - loss: 0.4122 - acc: 0.8135 - val_loss: 0.3438 - val_acc: 0.8585\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3820 - acc: 0.8274 - val_loss: 0.3476 - val_acc: 0.8615\n",
      "16000/16000 [==============================] - 2s 137us/step\n",
      "4000/4000 [==============================] - 1s 143us/step\n",
      "Train on 16000 samples, validate on 4000 samples\n",
      "Epoch 1/4\n",
      "16000/16000 [==============================] - 5s 311us/step - loss: 0.6808 - acc: 0.6187 - val_loss: 0.4890 - val_acc: 0.7597\n",
      "Epoch 2/4\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.5512 - acc: 0.7242 - val_loss: 0.3849 - val_acc: 0.8313\n",
      "Epoch 3/4\n",
      "16000/16000 [==============================] - 3s 168us/step - loss: 0.4757 - acc: 0.7748 - val_loss: 0.3583 - val_acc: 0.8477\n",
      "Epoch 4/4\n",
      "16000/16000 [==============================] - 3s 166us/step - loss: 0.4297 - acc: 0.8028 - val_loss: 0.3516 - val_acc: 0.8555\n",
      "16000/16000 [==============================] - 2s 140us/step\n",
      "4000/4000 [==============================] - 1s 141us/step\n"
     ]
    }
   ],
   "source": [
    "# Doing hyperparameter optimization\n",
    "dp = np.linspace(0,0.9,19)\n",
    "Vacc = []\n",
    "TrainErr = []\n",
    "TestErr = []\n",
    "for p in dp:\n",
    "    model = getModel([500,250,125],p)\n",
    "    # Compile it and fit\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "    model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=4,verbose=1,validation_data=(Xval, Yval))\n",
    "    TrainErr.append(model.evaluate(x=Xtrain, y=Ytrain))\n",
    "    TestErr.append(model.evaluate(x=Xval, y=Yval))\n",
    "TrainErr = np.array(TrainErr)\n",
    "TestErr = np.array(TestErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16327d4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.title('Dropout prob. vs accuracy')\n",
    "plt.xlabel('Dropout prob.')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(dp,TrainErr[:,1],label='Training',color='red')\n",
    "plt.plot(dp,TestErr[:,1],label='Validation',color='blue')\n",
    "plt.savefig('DNN_hyperparam.png')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([0., 1.]), array([0.6994479 , 0.30055207], dtype=float32), array(['thi', 'veri', 'get', 'onli', 'look', 'want', 'see', 'mani', 'new',\n",
      "       'peopl', 'still', 'need', 'two', 'feel', 'start', 'long', 'listen',\n",
      "       'excel', 'enough', 'person', 'cover', 'almost', 'scene', 'instead',\n",
      "       'famili', 'sever', 'hour', 'els', 'fine', 'talk', 'american',\n",
      "       'entir', 'lack', 'impress', 'state', 'avail', 'certainli',\n",
      "       'student', 'danc', 'parent', 'critic', 'centuri', 'train',\n",
      "       'aspect'], dtype='<U12'), array([3., 2., 1., 1., 2., 1., 2., 3., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       2., 1., 1., 1., 1., 2., 1., 1., 1., 1., 1., 1., 2., 1., 1., 1., 1.,\n",
      "       1., 1., 2., 2., 6., 1., 1., 1., 1., 1.])]\n"
     ]
    }
   ],
   "source": [
    "# Debugging bag of words\n",
    "out = getBagOfWords(Xval,ypred,Yval,words)\n",
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method  validation for single DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying Random Forest of weakly trained Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 666us/step - loss: 0.4852 - acc: 0.7666\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 167us/step - loss: 0.3227 - acc: 0.8596\n",
      "16000/16000 [==============================] - 6s 394us/step\n",
      "4000/4000 [==============================] - 1s 220us/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 9s 546us/step - loss: 0.4764 - acc: 0.7746\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.3151 - acc: 0.8611\n",
      "16000/16000 [==============================] - 7s 416us/step\n",
      "4000/4000 [==============================] - 1s 293us/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 661us/step - loss: 0.4841 - acc: 0.7692\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 168us/step - loss: 0.3177 - acc: 0.8638\n",
      "16000/16000 [==============================] - 6s 383us/step\n",
      "4000/4000 [==============================] - 1s 244us/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 9s 593us/step - loss: 0.4864 - acc: 0.7689\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3198 - acc: 0.8623\n",
      "16000/16000 [==============================] - 6s 390us/step\n",
      "4000/4000 [==============================] - 1s 227us/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 9s 593us/step - loss: 0.4747 - acc: 0.7714\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 169us/step - loss: 0.3197 - acc: 0.8619\n",
      "16000/16000 [==============================] - 6s 393us/step\n",
      "4000/4000 [==============================] - 1s 226us/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - ETA: 0s - loss: 0.4896 - acc: 0.767 - 10s 615us/step - loss: 0.4895 - acc: 0.7676\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.3176 - acc: 0.8626\n",
      "16000/16000 [==============================] - 6s 403us/step\n",
      "4000/4000 [==============================] - 1s 256us/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 612us/step - loss: 0.4723 - acc: 0.7731\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 187us/step - loss: 0.3180 - acc: 0.8626\n",
      "16000/16000 [==============================] - 7s 446us/step\n",
      "4000/4000 [==============================] - 1s 338us/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 728us/step - loss: 0.4767 - acc: 0.7732\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 4s 229us/step - loss: 0.3156 - acc: 0.8632\n",
      "16000/16000 [==============================] - 7s 455us/step\n",
      "4000/4000 [==============================] - 1s 189us/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 621us/step - loss: 0.4685 - acc: 0.7750\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 152us/step - loss: 0.3111 - acc: 0.8644\n",
      "16000/16000 [==============================] - 7s 441us/step\n",
      "4000/4000 [==============================] - 1s 235us/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 620us/step - loss: 0.4769 - acc: 0.7721\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 178us/step - loss: 0.3161 - acc: 0.8631\n",
      "16000/16000 [==============================] - 7s 429us/step\n",
      "4000/4000 [==============================] - 1s 258us/step\n",
      "Training DNN  10\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 661us/step - loss: 0.4855 - acc: 0.7702\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 201us/step - loss: 0.3193 - acc: 0.8589\n",
      "16000/16000 [==============================] - 7s 440us/step\n",
      "4000/4000 [==============================] - 1s 242us/step\n",
      "Training DNN  11\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 649us/step - loss: 0.4834 - acc: 0.7641\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 184us/step - loss: 0.3174 - acc: 0.8627\n",
      "16000/16000 [==============================] - 7s 428us/step\n",
      "4000/4000 [==============================] - 1s 235us/step\n",
      "Training DNN  12\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 631us/step - loss: 0.4754 - acc: 0.7717\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 172us/step - loss: 0.3227 - acc: 0.8598\n",
      "16000/16000 [==============================] - 7s 410us/step\n",
      "4000/4000 [==============================] - 1s 224us/step\n",
      "Training DNN  13\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 669us/step - loss: 0.4856 - acc: 0.7651\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 162us/step - loss: 0.3177 - acc: 0.8614\n",
      "16000/16000 [==============================] - 7s 446us/step\n",
      "4000/4000 [==============================] - 1s 253us/step\n",
      "Training DNN  14\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 662us/step - loss: 0.4895 - acc: 0.7658\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 183us/step - loss: 0.3224 - acc: 0.8623\n",
      "16000/16000 [==============================] - 7s 433us/step\n",
      "4000/4000 [==============================] - 1s 240us/step\n",
      "Training DNN  15\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 638us/step - loss: 0.4899 - acc: 0.7672\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 171us/step - loss: 0.3198 - acc: 0.8618\n",
      "16000/16000 [==============================] - 7s 433us/step\n",
      "4000/4000 [==============================] - 1s 269us/step\n",
      "Training DNN  16\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 10s 644us/step - loss: 0.4828 - acc: 0.7681\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3196 - acc: 0.8669\n",
      "16000/16000 [==============================] - 7s 440us/step\n",
      "4000/4000 [==============================] - 1s 246us/step\n",
      "Training DNN  17\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 687us/step - loss: 0.4901 - acc: 0.7662\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 187us/step - loss: 0.3262 - acc: 0.8556\n",
      "16000/16000 [==============================] - 7s 426us/step\n",
      "4000/4000 [==============================] - 1s 242us/step\n",
      "Training DNN  18\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 666us/step - loss: 0.4884 - acc: 0.7659\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 186us/step - loss: 0.3207 - acc: 0.8618\n",
      "16000/16000 [==============================] - 7s 450us/step\n",
      "4000/4000 [==============================] - 1s 237us/step\n",
      "Training DNN  19\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 696us/step - loss: 0.4841 - acc: 0.7682\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 171us/step - loss: 0.3198 - acc: 0.8622\n",
      "16000/16000 [==============================] - 7s 463us/step\n",
      "4000/4000 [==============================] - 1s 248us/step\n",
      "Training DNN  20\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 762us/step - loss: 0.4826 - acc: 0.7692\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 169us/step - loss: 0.3122 - acc: 0.8665\n",
      "16000/16000 [==============================] - 7s 446us/step\n",
      "4000/4000 [==============================] - 1s 247us/step\n",
      "Training DNN  21\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 678us/step - loss: 0.4832 - acc: 0.7712\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 175us/step - loss: 0.3167 - acc: 0.8635\n",
      "16000/16000 [==============================] - 7s 460us/step\n",
      "4000/4000 [==============================] - 1s 278us/step\n",
      "Training DNN  22\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 694us/step - loss: 0.4912 - acc: 0.7621\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 176us/step - loss: 0.3186 - acc: 0.8603\n",
      "16000/16000 [==============================] - 7s 449us/step\n",
      "4000/4000 [==============================] - 1s 253us/step\n",
      "Training DNN  23\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 693us/step - loss: 0.4958 - acc: 0.7643\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 173us/step - loss: 0.3202 - acc: 0.8644\n",
      "16000/16000 [==============================] - 8s 471us/step\n",
      "4000/4000 [==============================] - 1s 252us/step\n",
      "Training DNN  24\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 11s 673us/step - loss: 0.4900 - acc: 0.7640\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 178us/step - loss: 0.3209 - acc: 0.8605\n",
      "16000/16000 [==============================] - 7s 463us/step\n",
      "4000/4000 [==============================] - 1s 239us/step\n",
      "Training DNN  25\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 718us/step - loss: 0.4798 - acc: 0.7744\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 196us/step - loss: 0.3185 - acc: 0.8611\n",
      "16000/16000 [==============================] - 8s 479us/step\n",
      "4000/4000 [==============================] - 1s 297us/step\n",
      "Training DNN  26\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 695us/step - loss: 0.4902 - acc: 0.7641\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.3243 - acc: 0.8619\n",
      "16000/16000 [==============================] - 8s 492us/step\n",
      "4000/4000 [==============================] - 1s 246us/step\n",
      "Training DNN  27\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 777us/step - loss: 0.4869 - acc: 0.7678\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 210us/step - loss: 0.3176 - acc: 0.8639\n",
      "16000/16000 [==============================] - 9s 567us/step\n",
      "4000/4000 [==============================] - 1s 245us/step\n",
      "Training DNN  28\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 11s 718us/step - loss: 0.4852 - acc: 0.7682\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 191us/step - loss: 0.3191 - acc: 0.8599\n",
      "16000/16000 [==============================] - 8s 517us/step\n",
      "4000/4000 [==============================] - 1s 347us/step\n",
      "Training DNN  29\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 750us/step - loss: 0.4803 - acc: 0.7707\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 170us/step - loss: 0.3199 - acc: 0.8612\n",
      "16000/16000 [==============================] - 8s 494us/step\n",
      "4000/4000 [==============================] - 1s 287us/step\n",
      "Training DNN  30\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 752us/step - loss: 0.4809 - acc: 0.7706\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 182us/step - loss: 0.3197 - acc: 0.8618\n",
      "16000/16000 [==============================] - 8s 497us/step\n",
      "4000/4000 [==============================] - 1s 254us/step\n",
      "Training DNN  31\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 742us/step - loss: 0.4743 - acc: 0.7710\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 190us/step - loss: 0.3166 - acc: 0.8619\n",
      "16000/16000 [==============================] - 8s 477us/step\n",
      "4000/4000 [==============================] - 1s 263us/step\n",
      "Training DNN  32\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 13s 803us/step - loss: 0.4816 - acc: 0.7682\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 205us/step - loss: 0.3209 - acc: 0.8622\n",
      "16000/16000 [==============================] - 8s 517us/step\n",
      "4000/4000 [==============================] - 1s 280us/step\n",
      "Training DNN  33\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 867us/step - loss: 0.4830 - acc: 0.7655\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3161 - acc: 0.8641\n",
      "16000/16000 [==============================] - 9s 573us/step\n",
      "4000/4000 [==============================] - 1s 252us/step\n",
      "Training DNN  34\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 13s 821us/step - loss: 0.4767 - acc: 0.7731\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 4s 220us/step - loss: 0.3202 - acc: 0.8609\n",
      "16000/16000 [==============================] - 9s 562us/step\n",
      "4000/4000 [==============================] - 1s 274us/step\n",
      "Training DNN  35\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 13s 794us/step - loss: 0.4815 - acc: 0.7721\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 4s 227us/step - loss: 0.3183 - acc: 0.8616\n",
      "16000/16000 [==============================] - 11s 682us/step\n",
      "4000/4000 [==============================] - 1s 267us/step\n",
      "Training DNN  36\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 895us/step - loss: 0.4831 - acc: 0.7714\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 4s 262us/step - loss: 0.3167 - acc: 0.8641\n",
      "16000/16000 [==============================] - 8s 524us/step\n",
      "4000/4000 [==============================] - 1s 221us/step\n",
      "Training DNN  37\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 735us/step - loss: 0.4788 - acc: 0.7746\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 4s 257us/step - loss: 0.3210 - acc: 0.8609\n",
      "16000/16000 [==============================] - 9s 588us/step\n",
      "4000/4000 [==============================] - 1s 308us/step\n",
      "Training DNN  38\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 722us/step - loss: 0.4825 - acc: 0.7681\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3211 - acc: 0.8593\n",
      "16000/16000 [==============================] - 8s 483us/step\n",
      "4000/4000 [==============================] - 1s 255us/step\n",
      "Training DNN  39\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 729us/step - loss: 0.4911 - acc: 0.7641\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.3212 - acc: 0.8612\n",
      "16000/16000 [==============================] - 8s 487us/step\n",
      "4000/4000 [==============================] - 1s 256us/step\n",
      "Training DNN  40\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 731us/step - loss: 0.4744 - acc: 0.7715\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3133 - acc: 0.8641\n",
      "16000/16000 [==============================] - 8s 493us/step\n",
      "4000/4000 [==============================] - 1s 264us/step\n",
      "Training DNN  41\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 736us/step - loss: 0.4775 - acc: 0.7698\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3154 - acc: 0.8636\n",
      "16000/16000 [==============================] - 8s 497us/step\n",
      "4000/4000 [==============================] - 1s 266us/step\n",
      "Training DNN  42\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 748us/step - loss: 0.4852 - acc: 0.7638\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.3220 - acc: 0.8586\n",
      "16000/16000 [==============================] - 8s 498us/step\n",
      "4000/4000 [==============================] - 1s 264us/step\n",
      "Training DNN  43\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 744us/step - loss: 0.4848 - acc: 0.7691\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3223 - acc: 0.8594\n",
      "16000/16000 [==============================] - 8s 494us/step\n",
      "4000/4000 [==============================] - 1s 261us/step\n",
      "Training DNN  44\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 748us/step - loss: 0.5065 - acc: 0.7664\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.3191 - acc: 0.8630\n",
      "16000/16000 [==============================] - 8s 501us/step\n",
      "4000/4000 [==============================] - 1s 258us/step\n",
      "Training DNN  45\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 754us/step - loss: 0.4810 - acc: 0.7726\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3189 - acc: 0.8629\n",
      "16000/16000 [==============================] - 8s 501us/step\n",
      "4000/4000 [==============================] - 1s 278us/step\n",
      "Training DNN  46\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 768us/step - loss: 0.4928 - acc: 0.7638\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.3191 - acc: 0.8629\n",
      "16000/16000 [==============================] - 8s 503us/step\n",
      "4000/4000 [==============================] - 1s 263us/step\n",
      "Training DNN  47\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 765us/step - loss: 0.4806 - acc: 0.7702\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 166us/step - loss: 0.3119 - acc: 0.8677\n",
      "16000/16000 [==============================] - 8s 508us/step\n",
      "4000/4000 [==============================] - 1s 265us/step\n",
      "Training DNN  48\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000/16000 [==============================] - 12s 769us/step - loss: 0.4757 - acc: 0.7714\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 148us/step - loss: 0.3113 - acc: 0.8654\n",
      "16000/16000 [==============================] - 7s 459us/step\n",
      "4000/4000 [==============================] - 1s 212us/step\n",
      "Training DNN  49\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 12s 752us/step - loss: 0.4843 - acc: 0.7681\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 145us/step - loss: 0.3182 - acc: 0.8632\n",
      "16000/16000 [==============================] - 8s 477us/step\n",
      "4000/4000 [==============================] - 1s 214us/step\n"
     ]
    }
   ],
   "source": [
    "# Split the training data k-fold number of ways for k-fold validation of the learning algorithm\n",
    "k=5\n",
    "kf = sklearn.model_selection.KFold(n_splits=k)\n",
    "inds = [ind for ind in kf.split(x_tall, y_tall)]\n",
    "\n",
    "train,val = inds[0]\n",
    "# Training and validation data for k fold cross validation\n",
    "Xtrain = x_tall[train]\n",
    "Ytrain = y_tall[train]\n",
    "Xval = x_tall[val]\n",
    "Yval = y_tall[val]\n",
    "\n",
    "# Specify number of Neural networks to train\n",
    "N_models = 50\n",
    "Predictions = []\n",
    "TrainErr = []\n",
    "TestErr = []\n",
    "\n",
    "# Define the DNN model\n",
    "for i in range(0,N_models):\n",
    "    print('Training DNN ',i)\n",
    "\n",
    "    # Generate model and fit to data\n",
    "    model = getModel([500,250,125],0.4)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "    model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=2)\n",
    "\n",
    "    # Store the models\n",
    "    TrainErr.append(model.evaluate(x=Xtrain, y=Ytrain))\n",
    "    TestErr.append(model.evaluate(x=Xval, y=Yval))\n",
    "    # Use weakly trained model to predict and store predictions\n",
    "    ypred = model.predict(Xval,batch_size=2**8)\n",
    "    Predictions.append(ypred)\n",
    "\n",
    "Predictions = np.array(Predictions)\n",
    "TrainErr = np.array(TrainErr)\n",
    "TestErr = np.array(TestErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples where stdev > 0,  1382\n",
      "Final accuracy of random forest =  0.863\n"
     ]
    }
   ],
   "source": [
    "ypred = []\n",
    "for i in Predictions:\n",
    "    ypred.append(Unencode(i).astype(int))\n",
    "\n",
    "# Get mean and standard deviation of samples\n",
    "ypmean=np.mean(ypred,axis=0)\n",
    "std=np.std(ypred,axis=0)\n",
    "\n",
    "print('Number of samples where stdev > 0, ',np.sum(std>0))\n",
    "ypred = (ypmean > 0.5).astype(int)\n",
    "ytrue = Unencode(Yval).astype(int)\n",
    "\n",
    "acc = 1.0*np.sum(ypred == ytrue)/len(ytrue)\n",
    "print('Final accuracy of random forest = ',acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 3)\n",
      "[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48\n",
      " 49 50]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/usr/local/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (49,) and (50,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-564ad9307825>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'N ensemble Neural Networks'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Avg training accuracy of individual neuron'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Avg test accuracy of individual neuron'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Ensemble accuracy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'red'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3259\u001b[0m                       mplDeprecation)\n\u001b[1;32m   3260\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3261\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3262\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3263\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwashold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1715\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1716\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1717\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1718\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             raise ValueError(\"x and y must have same first dimension, but \"\n\u001b[0;32m--> 243\u001b[0;31m                              \"have shapes {} and {}\".format(x.shape, y.shape))\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             raise ValueError(\"x and y can be no greater than 2-D, but have \"\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (49,) and (50,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21f9baa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to get explicit ensemble model accuracy from softmax\n",
    "def getEnsembleAccuracy(Predictions,Yval):\n",
    "    ypred = []\n",
    "    # undo one hot encoding\n",
    "    for i in Predictions:\n",
    "        ypred.append(Unencode(i).astype(int))\n",
    "    # Average the predictions and get final predictions\n",
    "    ypmean=np.mean(ypred,axis=0)\n",
    "    ypred = (ypmean > 0.5).astype(int)\n",
    "    \n",
    "    # Get explicit accuracy\n",
    "    ytrue = Unencode(Yval).astype(int)\n",
    "    acc = 1.0*np.sum(ypred == ytrue)/len(ytrue)\n",
    "    return acc\n",
    "\n",
    "Predictions = np.array(Predictions)\n",
    "acc = []\n",
    "for i in range(1,len(Predictions)):\n",
    "    acc.append([np.mean(TrainErr[:i,1]),np.mean(TestErr[:i,1]),getEnsembleAccuracy(Predictions[:i,:],Yval)])\n",
    "\n",
    "plt.figure(2)\n",
    "plt.title('Bagging the neural networks')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('N ensemble Neural Networks')\n",
    "plt.plot(np.arange(1,51),acc[:,0],label='Avg training accuracy of individual neuron',color='red')\n",
    "plt.plot(np.arange(1,51),acc[:,1],label='Avg test accuracy of individual neuron',color='red')\n",
    "plt.plot(np.arange(1,51),acc[:,2],label='Ensemble accuracy',color='red')\n",
    "plt.legend()\n",
    "plt.savefig('NNbag.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random neuron forest\n",
    "def getDNNForestCross(k,N_models,x_tall,y_tall):\n",
    "    # Storage for k fold cross validation\n",
    "    facc = []\n",
    "    \n",
    "    # Split the training data k-fold number of ways for k-fold validation of the learning algorithm\n",
    "    k=5\n",
    "    kf = sklearn.model_selection.KFold(n_splits=k)\n",
    "    inds = [ind for ind in kf.split(x_tall, y_tall)]\n",
    "\n",
    "    j=0\n",
    "    for train,val in inds:\n",
    "        print('Cross fold validation ',j+1,'/',k)\n",
    "        # Training and validation data for k fold cross validation\n",
    "        Xtrain = x_tall[train]\n",
    "        Ytrain = y_tall[train]\n",
    "        Xval = x_tall[val]\n",
    "        Yval = y_tall[val]\n",
    "\n",
    "        # Store predictions from each tree in DNN forest\n",
    "        Predictions = []\n",
    "        \n",
    "        # Define the DNN model\n",
    "        for i in range(0,N_models):\n",
    "            print('Training DNN ',i)\n",
    "            model = getModel([500,250,125],0)\n",
    "            # Compile it and fit\n",
    "            model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "            model.fit(Xtrain, Ytrain, batch_size=2**8, epochs=2,verbose=1)\n",
    "\n",
    "            # Use weakly trained model to predict and store predictions\n",
    "            ypred = model.predict(Xval,batch_size=2**8,verbose=1)\n",
    "            Predictions.append(ypred)\n",
    "\n",
    "        Predictions = np.array(Predictions)\n",
    "        \n",
    "        ypred = []\n",
    "        for i in Predictions:\n",
    "            ypred.append(Unencode(i).astype(int))\n",
    "\n",
    "        # Get mean and standard deviation of samples\n",
    "        ypmean=np.mean(ypred,axis=0)\n",
    "        std=np.std(ypred,axis=0)\n",
    "        print('Number of samples where stdev > 0, ',np.sum(std>0))\n",
    "        \n",
    "        # Compute accuracy of predictions\n",
    "        ypred = (ypmean > 0.5).astype(int)\n",
    "        ytrue = Unencode(Yval).astype(int)\n",
    "        acc = 1.0*np.sum(ypred == ytrue)/len(ytrue)\n",
    "        print('Final accuracy of random forest = ',acc)\n",
    "        j=j+1\n",
    "        facc.append(acc)\n",
    "        \n",
    "    print('avg Accuracy of all random forests',np.mean(facc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross fold validation  0 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 13s 797us/step - loss: 0.4342 - acc: 0.8000\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 154us/step - loss: 0.1825 - acc: 0.9311\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 844us/step - loss: 0.4393 - acc: 0.7969\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 181us/step - loss: 0.1899 - acc: 0.9297\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 13s 813us/step - loss: 0.4315 - acc: 0.8037\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 197us/step - loss: 0.1785 - acc: 0.9337\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 849us/step - loss: 0.4482 - acc: 0.7908\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 161us/step - loss: 0.1801 - acc: 0.9327\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 914us/step - loss: 0.4264 - acc: 0.8039\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 167us/step - loss: 0.1872 - acc: 0.9296\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 868us/step - loss: 0.4354 - acc: 0.8004\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.1803 - acc: 0.9325\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 901us/step - loss: 0.4466 - acc: 0.7948\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1833 - acc: 0.9324 1s - los\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 877us/step - loss: 0.4276 - acc: 0.8001\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 193us/step - loss: 0.1797 - acc: 0.9323\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 926us/step - loss: 0.4320 - acc: 0.8007\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 168us/step - loss: 0.1847 - acc: 0.9292\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 918us/step - loss: 0.4365 - acc: 0.7991\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.1814 - acc: 0.9316\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Number of samples where stdev > 0,  1412\n",
      "Final accuracy of random forest =  0.85875\n",
      "Cross fold validation  1 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 953us/step - loss: 0.4319 - acc: 0.8009\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 170us/step - loss: 0.1727 - acc: 0.9356\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 14s 895us/step - loss: 0.4277 - acc: 0.8003\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 155us/step - loss: 0.1778 - acc: 0.9345\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 956us/step - loss: 0.4280 - acc: 0.7999\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 178us/step - loss: 0.1737 - acc: 0.9371\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 974us/step - loss: 0.4380 - acc: 0.7984\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 179us/step - loss: 0.1732 - acc: 0.9376\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 926us/step - loss: 0.4274 - acc: 0.8024\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 174us/step - loss: 0.1778 - acc: 0.9344\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 942us/step - loss: 0.4268 - acc: 0.8018\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 181us/step - loss: 0.1781 - acc: 0.9345\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 954us/step - loss: 0.4271 - acc: 0.8026\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 183us/step - loss: 0.1814 - acc: 0.9299\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 922us/step - loss: 0.4383 - acc: 0.7955\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 157us/step - loss: 0.1805 - acc: 0.9323\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 985us/step - loss: 0.4259 - acc: 0.8011\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 203us/step - loss: 0.1843 - acc: 0.9308\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 933us/step - loss: 0.4337 - acc: 0.8018\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 214us/step - loss: 0.1791 - acc: 0.9334\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Number of samples where stdev > 0,  1477\n",
      "Final accuracy of random forest =  0.8565\n",
      "Cross fold validation  2 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 943us/step - loss: 0.4328 - acc: 0.7986\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 176us/step - loss: 0.1779 - acc: 0.9344\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 916us/step - loss: 0.4273 - acc: 0.8040\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 172us/step - loss: 0.1764 - acc: 0.9368\n",
      "4000/4000 [==============================] - 5s 1ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 926us/step - loss: 0.4320 - acc: 0.8034\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 174us/step - loss: 0.1699 - acc: 0.9351\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4280 - acc: 0.8054\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 215us/step - loss: 0.1779 - acc: 0.9364\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 949us/step - loss: 0.4283 - acc: 0.8006\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1789 - acc: 0.9339\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 999us/step - loss: 0.4344 - acc: 0.7976\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 164us/step - loss: 0.1716 - acc: 0.9378\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 934us/step - loss: 0.4269 - acc: 0.8023\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 165us/step - loss: 0.1805 - acc: 0.9324\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 940us/step - loss: 0.4355 - acc: 0.7994\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 170us/step - loss: 0.1769 - acc: 0.9353\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4364 - acc: 0.8008\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 194us/step - loss: 0.1781 - acc: 0.9326\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4347 - acc: 0.7999\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 158us/step - loss: 0.1776 - acc: 0.9346\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Number of samples where stdev > 0,  1377\n",
      "Final accuracy of random forest =  0.84375\n",
      "Cross fold validation  3 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 979us/step - loss: 0.4330 - acc: 0.7994\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 177us/step - loss: 0.1770 - acc: 0.9359\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 948us/step - loss: 0.4278 - acc: 0.8039\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 143us/step - loss: 0.1769 - acc: 0.9340\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4365 - acc: 0.7973\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 167us/step - loss: 0.1760 - acc: 0.9360\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4252 - acc: 0.8042\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.1731 - acc: 0.9356\n",
      "4000/4000 [==============================] - 6s 1ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 15s 966us/step - loss: 0.4374 - acc: 0.7981\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 182us/step - loss: 0.1804 - acc: 0.9327\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4335 - acc: 0.7999\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 2s 151us/step - loss: 0.1743 - acc: 0.9341\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4247 - acc: 0.8025\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 5s 301us/step - loss: 0.1781 - acc: 0.9342\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4332 - acc: 0.8029\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 4s 241us/step - loss: 0.1710 - acc: 0.9393\n",
      "4000/4000 [==============================] - 7s 2ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 19s 1ms/step - loss: 0.4344 - acc: 0.8004\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 193us/step - loss: 0.1707 - acc: 0.9379 1s - loss: \n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4299 - acc: 0.8013\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 174us/step - loss: 0.1760 - acc: 0.9359\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Number of samples where stdev > 0,  1400\n",
      "Final accuracy of random forest =  0.84675\n",
      "Cross fold validation  4 / 5\n",
      "Training DNN  0\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4308 - acc: 0.7984\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 167us/step - loss: 0.1777 - acc: 0.9331\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  1\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 999us/step - loss: 0.4294 - acc: 0.8018\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 163us/step - loss: 0.1806 - acc: 0.9347\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  2\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4283 - acc: 0.8021\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.1834 - acc: 0.9310\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  3\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4428 - acc: 0.7974\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 159us/step - loss: 0.1747 - acc: 0.9364\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  4\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 16s 1ms/step - loss: 0.4416 - acc: 0.7973\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 157us/step - loss: 0.1758 - acc: 0.9333\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  5\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 18s 1ms/step - loss: 0.4360 - acc: 0.8007\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 170us/step - loss: 0.1778 - acc: 0.9332\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  6\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 20s 1ms/step - loss: 0.4361 - acc: 0.7977\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 178us/step - loss: 0.1772 - acc: 0.9343\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  7\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4423 - acc: 0.7996\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 181us/step - loss: 0.1736 - acc: 0.9357\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  8\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4370 - acc: 0.7953\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 161us/step - loss: 0.1745 - acc: 0.9354\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Training DNN  9\n",
      "Epoch 1/2\n",
      "16000/16000 [==============================] - 17s 1ms/step - loss: 0.4249 - acc: 0.8009\n",
      "Epoch 2/2\n",
      "16000/16000 [==============================] - 3s 160us/step - loss: 0.1794 - acc: 0.9340\n",
      "4000/4000 [==============================] - 6s 2ms/step\n",
      "Number of samples where stdev > 0,  1661\n",
      "Final accuracy of random forest =  0.8485\n",
      "avg Accuracy of all random forests 0.8508500000000001\n"
     ]
    }
   ],
   "source": [
    "getDNNForestCross(5,10,x_tall,y_tall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output final predictions from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify number of Neural networks to train\n",
    "N_models = 20\n",
    "Predictions = []\n",
    "\n",
    "# Define the DNN model\n",
    "for i in range(0,N_models):\n",
    "    print('Training DNN ',i)\n",
    "    model = getModel([500,250,125],0.4)\n",
    "    # Compile it and fit\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='RMSprop', metrics=['accuracy'])\n",
    "    model.fit(x_tall, y_tall, batch_size=2**8, epochs=2)\n",
    "    # Use weakly trained model to predict and store predictions\n",
    "    ypred = model.predict(FV_data,batch_size=2**8)\n",
    "    Predictions.append(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predictions = np.array(Predictions)\n",
    "\n",
    "ypred = []\n",
    "for i in Predictions:\n",
    "    ypred.append(Unencode(i).astype(int))\n",
    "\n",
    "# Get mean and standard deviation of samples\n",
    "ypmean=np.mean(ypred,axis=0)\n",
    "std=np.std(ypred,axis=0)\n",
    "print('Number of samples where stdev > 0, ',np.sum(std>0))\n",
    "\n",
    "# Compute final predictions and output it\n",
    "ypred = (ypmean > 0.5).astype(int)\n",
    "writeResults(ypred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
